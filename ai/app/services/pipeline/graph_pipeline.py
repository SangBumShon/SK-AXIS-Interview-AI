"""
SK AXIS AI ë©´ì ‘ í‰ê°€ íŒŒì´í”„ë¼ì¸ - ê·¸ë˜í”„ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°

ì´ íŒŒì¼ì€ LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ ë©´ì ‘ í‰ê°€ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ê´€ë¦¬í•˜ëŠ” í•µì‹¬ ëª¨ë“ˆì…ë‹ˆë‹¤.
ì£¼ìš” ê¸°ëŠ¥:
- STT â†’ ë¦¬ë¼ì´íŒ… â†’ í‰ê°€ â†’ ìš”ì•½ê¹Œì§€ ì „ì²´ ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
- ê° ë‹¨ê³„ë³„ ê²€ì¦ ë° ì¬ì‹œë„ ë¡œì§ (ìµœëŒ€ 1íšŒ)
- ìƒíƒœ ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ ì§„í–‰ ìƒí™© ì¶”ì 
- ë¹„ìš© ìµœì í™”ëœ GPT-4o-mini ëª¨ë¸ ì‚¬ìš©

íŒŒì´í”„ë¼ì¸ êµ¬ì¡°:
1. STT Node: ìŒì„± â†’ í…ìŠ¤íŠ¸ ë³€í™˜
2. Rewrite Agent: í…ìŠ¤íŠ¸ ì •ì œ ë° ë¬¸ë²• ìˆ˜ì •
3. Rewrite Judge: ì •ì œ ê²°ê³¼ í’ˆì§ˆ ê²€ì¦
4. Nonverbal Evaluation: í‘œì • ê¸°ë°˜ ë¹„ì–¸ì–´ì  í‰ê°€
5. Evaluation Agent: 8ê°œ í‚¤ì›Œë“œ Ã— 3ê°œ ê¸°ì¤€ = 24ê°œ í•­ëª© í‰ê°€
6. Evaluation Judge: í‰ê°€ ê²°ê³¼ ê²€ì¦ ë° ë‚´ìš© ê²€ì¦
7. Score Summary: 100ì  ë§Œì  í™˜ì‚° ë° ìµœì¢… ìš”ì•½

ì„±ëŠ¥ ìµœì í™”:
- GPT-4o â†’ GPT-4o-mini ë³€ê²½ìœ¼ë¡œ 94% ë¹„ìš© ì ˆê°
- ì¬ì‹œë„ ë¡œì§ 1íšŒ ì œí•œìœ¼ë¡œ ë¬´í•œ ë£¨í”„ ë°©ì§€
- íŒŒì¼ í—¤ë” ê²€ì¦ìœ¼ë¡œ 3000ë°° ì†ë„ í–¥ìƒ
"""

# app/services/pipeline/graph_pipeline.py

from langgraph.graph import StateGraph
from datetime import datetime
from typing import Literal, Dict, Any
import os
import json
import openai
from dotenv import load_dotenv
import openpyxl
import httpx
import pytz

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ” í™˜ê²½ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
RESULT_DIR = os.getenv("RESULT_DIR", "./result")

from app.services.interview.stt_service import transcribe_audio_file
from app.services.interview.rewrite_service import rewrite_answer
from app.services.interview.evaluation_service import evaluate_keywords_from_full_answer
from app.services.interview.report_service import generate_pdf
from app.schemas.nonverbal import Posture, FacialExpression, NonverbalData
from app.services.interview.nonverbal_service import evaluate
from app.schemas.state import InterviewState
from langgraph.channels import LastValue, BinaryOperatorAggregate
from app.constants.evaluation_constants_full_all import (
    EVAL_CRITERIA_WITH_ALL_SCORES,
    TECHNICAL_EVAL_CRITERIA_WITH_ALL_SCORES,
    DOMAIN_EVAL_CRITERIA_WITH_ALL_SCORES
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ§  GPT í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¦¬ë¼ì´íŒ… ê²€ì¦ìš© í”„ë¡¬í”„íŠ¸ (í˜„ì¬ ì‚¬ìš© ì•ˆ í•¨ - ì¬ì‹œë„ ë¡œì§ ë¹„í™œì„±í™”)
JUDGE_PROMPT = """
ì‹œìŠ¤í…œ: ë‹¹ì‹ ì€ í…ìŠ¤íŠ¸ ë¦¬ë¼ì´íŒ… í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì›ë³¸: "{raw}"
ë¦¬ë¼ì´íŒ…: "{rewritten}"
1) ì˜ë¯¸ ë³´ì¡´
2) ê³¼ì‰ ì¶•ì•½/í™•ì¥
3) ì˜¤íƒˆì/ë¬¸ë§¥ ì˜¤ë¥˜
ìœ„ ê¸°ì¤€ì— ë”°ë¼ JSON í˜•ì‹ìœ¼ë¡œ ok(bool)ì™€ judge_notes(list)ë¥¼ ë°˜í™˜í•˜ì„¸ìš”.
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ› ï¸ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
KST = pytz.timezone('Asia/Seoul')

def print_state_summary(state, node_name):
    """
    íŒŒì´í”„ë¼ì¸ ìƒíƒœ ìš”ì•½ ì¶œë ¥ í•¨ìˆ˜ (ë””ë²„ê¹…ìš©)
    
    Args:
        state: í˜„ì¬ íŒŒì´í”„ë¼ì¸ ìƒíƒœ
        node_name: í˜„ì¬ ë…¸ë“œ ì´ë¦„
        
    Note:
        - ê° ë‹¨ê³„ë³„ ì²˜ë¦¬ ìƒíƒœ ë° ë°ì´í„° íƒ€ì… í™•ì¸
        - ë””ë²„ê¹… ì‹œ ìƒíƒœ ì¶”ì ì— ìœ ìš©
    """
    summary = {
        "stt_segments": len(state.get("stt", {}).get("segments", [])),
        "stt_type": type(state.get("stt", {})).__name__,
        "rewrite_final": len(state.get("rewrite", {}).get("final", [])),
        "rewrite_type": type(state.get("rewrite", {})).__name__,
        "evaluation_keys": list(state.get("evaluation", {}).keys()) if isinstance(state.get("evaluation", {}), dict) else [],
        "evaluation_type": type(state.get("evaluation", {})).__name__,
        "nonverbal_counts": state.get("nonverbal_counts", {}),
        "nonverbal_counts_type": type(state.get("nonverbal_counts", {})).__name__,
        "report_keys": list(state.get("report", {}).keys()) if "report" in state and isinstance(state["report"], dict) else [],
        "report_type": type(state.get("report", {})).__name__ if "report" in state else None,
        "decision_log_len": len(state.get("decision_log", [])),
        "decision_log_type": type(state.get("decision_log", [])).__name__
    }


def safe_get(d, key, default=None, context=""):
    """
    ì•ˆì „í•œ ë”•ì…”ë„ˆë¦¬ ì ‘ê·¼ í•¨ìˆ˜
    
    Args:
        d: ë”•ì…”ë„ˆë¦¬ ê°ì²´
        key: ì ‘ê·¼í•  í‚¤
        default: ê¸°ë³¸ê°’
        context: ì—ëŸ¬ ë°œìƒ ì‹œ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
        
    Returns:
        ë”•ì…”ë„ˆë¦¬ ê°’ ë˜ëŠ” ê¸°ë³¸ê°’
        
    Note:
        - ì˜ˆì™¸ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        - ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¡œ ì—ëŸ¬ ì¶”ì  ê°€ëŠ¥
    """
    try:
        return d.get(key, default)
    except Exception as e:
        print(f"[ERROR] [{context}] get('{key}') ì˜ˆì™¸ ë°œìƒ: {e}")
        return default
    

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ¯ íŒŒì´í”„ë¼ì¸ ë…¸ë“œ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) STT ë…¸ë“œ: audio_path â†’ raw text
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def stt_node(state: InterviewState) -> InterviewState:
    """
    ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” STT ë…¸ë“œ
    
    Args:
        state (InterviewState): ë©´ì ‘ ìƒíƒœ ê°ì²´
        
    Returns:
        InterviewState: STT ê²°ê³¼ê°€ ì¶”ê°€ëœ ìƒíƒœ
        
    ì²˜ë¦¬ ê³¼ì •:
    1. audio_pathì—ì„œ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ì¶”ì¶œ
    2. OpenAI Whisper APIë¡œ ìŒì„± ì¸ì‹ ìˆ˜í–‰
    3. ì†ìƒëœ íŒŒì¼ ë˜ëŠ” ì¸ì‹ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë©”ì‹œì§€ ì„¤ì •
    4. ê²°ê³¼ë¥¼ state["stt"]["segments"]ì— ì €ì¥
    
    Note:
        - íŒŒì¼ í—¤ë” ê²€ì¦ìœ¼ë¡œ 3000ë°° ì†ë„ í–¥ìƒ
        - ì†ìƒëœ WebM íŒŒì¼ ì‚¬ì „ ê°ì§€
        - ìœ íŠœë¸Œ ê´€ë ¨ ì˜¤ì¸ì‹ í•„í„°ë§
    """
    print("[LangGraph] ğŸ§  stt_node ì§„ì…")
    
    audio_path = safe_get(state, "audio_path", context="stt_node")
    raw = transcribe_audio_file(audio_path)
    
    # ì†ìƒëœ íŒŒì¼ ë˜ëŠ” STT ì‹¤íŒ¨ ì²˜ë¦¬
    if not raw or not str(raw).strip():
        raw = "ìŒì„±ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    elif "ì†ìƒë˜ì–´ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in raw:
        print(f"[LangGraph] âš ï¸ ì†ìƒëœ ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬: {audio_path}")
        # ì†ìƒëœ íŒŒì¼ì— ëŒ€í•œ ê¸°ë³¸ ë‹µë³€ ì„¤ì •
        raw = "ê¸°ìˆ ì  ë¬¸ì œë¡œ ìŒì„±ì„ ì¸ì‹í•  ìˆ˜ ì—†ì–´ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    state.setdefault("stt", {"done": False, "segments": []})
    state["stt"]["segments"].append({"raw": raw, "timestamp": datetime.now(KST).isoformat()})
    
    print(f"[LangGraph] âœ… STT ì™„ë£Œ: {raw[:50]}...")
    return state

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2) Rewrite ì—ì´ì „íŠ¸: raw â†’ rewritten
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def rewrite_agent(state: InterviewState) -> InterviewState:
    """
    STT ê²°ê³¼ë¥¼ ë¬¸ë²•ì ìœ¼ë¡œ ì •ì œí•˜ëŠ” ë¦¬ë¼ì´íŒ… ì—ì´ì „íŠ¸
    
    Args:
        state (InterviewState): ë©´ì ‘ ìƒíƒœ ê°ì²´
        
    Returns:
        InterviewState: ë¦¬ë¼ì´íŒ… ê²°ê³¼ê°€ ì¶”ê°€ëœ ìƒíƒœ
        
    ì²˜ë¦¬ ê³¼ì •:
    1. STT ê²°ê³¼ì—ì„œ ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ
    2. GPT-4o-minië¡œ ì˜ë¯¸ ë³´ì¡´ ê¸°ë°˜ í…ìŠ¤íŠ¸ ì •ì œ
    3. ì¬ì‹œë„ íšŸìˆ˜ ê´€ë¦¬ (í˜„ì¬ ì¬ì‹œë„ ë¹„í™œì„±í™”)
    4. ê²°ê³¼ë¥¼ state["rewrite"]["items"]ì— ì €ì¥
    
    Note:
        - ì§€ì›ì ë‹µë³€ ì˜ë¯¸ ì ˆëŒ€ ë³€ê²½ ì•ˆ í•¨
        - ë¬¸ë²• ì˜¤ë¥˜ ë° ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        - ë©´ì ‘ê´€ ë°œì–¸ í•„í„°ë§
        - GPT-4o-mini ì‚¬ìš©ìœ¼ë¡œ ë¹„ìš© ì ˆì•½
    """
    print("[LangGraph] âœï¸ rewrite_agent ì§„ì…")
    stt = safe_get(state, "stt", {}, context="rewrite_agent")
    stt_segments = safe_get(stt, "segments", [], context="rewrite_agent")
    raw = stt_segments[-1]["raw"] if stt_segments else "ì—†ìŒ"
    if not raw or not str(raw).strip():
        raw = "ì—†ìŒ"
    rewritten, _ = await rewrite_answer(raw)
    if not rewritten or not str(rewritten).strip():
        rewritten = "ì—†ìŒ"
    item = {"raw": raw, "rewritten": rewritten}

    prev = safe_get(state, "rewrite", {}, context="rewrite_agent")
    prev_retry = safe_get(prev, "retry_count", 0, context="rewrite_agent")
    prev_force = safe_get(prev, "force_ok", False, context="rewrite_agent")
    prev_final = safe_get(prev, "final", [], context="rewrite_agent")

    # retry_countê°€ 3 ì´ìƒì´ë©´ ë” ì´ìƒ ì¦ê°€ì‹œí‚¤ì§€ ì•ŠìŒ
    if prev_retry >= 3:
        retry_count = prev_retry
    elif prev.get("items") and "ok" in prev["items"][0] and not prev["items"][0]["ok"]:
        retry_count = prev_retry + 1
    else:
        retry_count = prev_retry

    state["rewrite"] = {
        "items":       [item],
        "retry_count": retry_count,
        "force_ok":    prev_force,
        "final":       prev_final,
        "done":        False
    }

    print(f"[LangGraph] âœ… rewrite ê²°ê³¼: {rewritten[:30]}... (retry_count={retry_count})")
    ts = datetime.now(KST).isoformat()
    state.setdefault("decision_log", []).append({
        "step":   "rewrite_agent",
        "result": "processing",
        "time":   ts,
        "details": {"raw_preview": raw[:30], "retry_count": retry_count}
    })
    # print_state_summary(state, "rewrite_agent")
    return state

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3) Rewrite ì¬ì‹œë„ ì¡°ê±´: ìµœëŒ€ 3íšŒ â†’ ë‹¨ 1íšŒë§Œ ìˆ˜í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def should_retry_rewrite(state: InterviewState) -> Literal["retry", "done"]:
    """
    ë¦¬ë¼ì´íŒ… ì¬ì‹œë„ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ëŠ” ì¡°ê±´ í•¨ìˆ˜
    
    Args:
        state (InterviewState): ë©´ì ‘ ìƒíƒœ ê°ì²´
        
    Returns:
        Literal["retry", "done"]: ì¬ì‹œë„ ë˜ëŠ” ì™„ë£Œ
        
    Note:
        - í˜„ì¬ ì¬ì‹œë„ ë¡œì§ ë¹„í™œì„±í™” (ë¹„ìš© ì ˆì•½)
        - í•­ìƒ "done" ë°˜í™˜í•˜ì—¬ í•œ ë²ˆë§Œ ì‹¤í–‰
        - í•„ìš” ì‹œ ì¬ì‹œë„ ë¡œì§ í™œì„±í™” ê°€ëŠ¥
    """
    # í•­ìƒ done ë°˜í™˜ (ì¬ì‹œë„ ì—†ìŒ)
    return "done"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4) Rewrite ê²€ì¦ ì—ì´ì „íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def rewrite_judge_agent(state: InterviewState) -> InterviewState:
    """
    ë¦¬ë¼ì´íŒ… ê²°ê³¼ë¥¼ ê²€ì¦í•˜ëŠ” íŒì • ì—ì´ì „íŠ¸
    
    Args:
        state (InterviewState): ë©´ì ‘ ìƒíƒœ ê°ì²´
        
    Returns:
        InterviewState: ê²€ì¦ ê²°ê³¼ê°€ ì¶”ê°€ëœ ìƒíƒœ
        
    ì²˜ë¦¬ ê³¼ì •:
    1. ë¦¬ë¼ì´íŒ… ê²°ê³¼ í’ˆì§ˆ ê²€ì¦ (ì˜ë¯¸ ë³´ì¡´, ë¬¸ë²• ì •í™•ì„±)
    2. GPT-4o-minië¡œ íŒì • ìˆ˜í–‰
    3. ê²€ì¦ í†µê³¼ ì‹œ final ë°°ì—´ì— ì¶”ê°€
    4. ê°•ì œ í†µê³¼ í”Œë˜ê·¸ ì²˜ë¦¬
    
    Note:
        - í˜„ì¬ ì¬ì‹œë„ ë¡œì§ ë¹„í™œì„±í™”ë¡œ ëŒ€ë¶€ë¶„ í†µê³¼
        - JSON íŒŒì‹± ì˜¤ë¥˜ ì‹œ ì•ˆì „ ì²˜ë¦¬
        - ì¤‘ë³µ ë‹µë³€ í•„í„°ë§ ë¡œì§ í¬í•¨
    """
    print("[LangGraph] ğŸ§ª rewrite_judge_agent ì§„ì…")
    rewrite = safe_get(state, "rewrite", {}, context="rewrite_judge_agent")
    items   = safe_get(rewrite, "items", [])
    force   = safe_get(rewrite, "force_ok", False, context="rewrite_judge_agent")

    if not items:
        state.setdefault("decision_log", []).append({
            "step":   "rewrite_judge_agent",
            "result": "error",
            "time":   datetime.now(KST).isoformat(),
            "details":{"error":"No rewrite items found"}
        })
        return state

    for item in items:
        if "ok" in item:
            continue

        prompt = JUDGE_PROMPT.format(raw=item["raw"], rewritten=item["rewritten"])
        print(f"[DEBUG] ğŸ” Rewrite íŒì • í”„ë¡¬í”„íŠ¸:")
        print(f"ì›ë³¸: {item['raw'][:100]}...")
        print(f"ë¦¬ë¼ì´íŒ…: {item['rewritten'][:100]}...")
        
        try:
            start = datetime.now(KST).timestamp()
            resp  = openai.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role":"user","content":prompt}],
                temperature=0, max_tokens=512
            )
            elapsed = datetime.now(KST).timestamp() - start
            
            # LLM ì‘ë‹µ ë¡œê·¸
            llm_response = resp.choices[0].message.content.strip()
            print(f"[DEBUG] ğŸ¤– LLM ì‘ë‹µ: {llm_response}")
            
            # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
            if llm_response.startswith("```json"):
                llm_response = llm_response[7:]  # "```json" ì œê±°
            if llm_response.startswith("```"):
                llm_response = llm_response[3:]   # "```" ì œê±°
            if llm_response.endswith("```"):
                llm_response = llm_response[:-3]  # ëì˜ "```" ì œê±°
            
            llm_response = llm_response.strip()
            print(f"[DEBUG] ğŸ”§ ì •ë¦¬ëœ JSON: {llm_response}")
            
            result  = json.loads(llm_response)

            item["ok"]          = result.get("ok", False)
            item["judge_notes"] = result.get("judge_notes", [])

            print(f"[DEBUG] ğŸ“Š íŒì • ê²°ê³¼: ok={item['ok']}, notes={item['judge_notes']}")

            # ê°•ì œ í†µê³¼ í”Œë˜ê·¸ ì²˜ë¦¬
            if not item["ok"] and force:
                print("âš ï¸ rewrite ì‹¤íŒ¨ í•­ëª© ê°•ì œ ok ì²˜ë¦¬ë¨")
                item["ok"] = True
                item["judge_notes"].append("ê°•ì œ í†µê³¼ (ì¬ì‹œë„ 3íšŒ ì´ˆê³¼)")

            if item["ok"]:
                # ì¤‘ë³µëœ rewritten ë‹µë³€ì´ ì´ë¯¸ finalì— ìˆìœ¼ë©´ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
                rewritten = item["rewritten"]
                if not any(f["rewritten"] == rewritten for f in rewrite.get("final", [])):
                    rewrite.setdefault("final", []).append({
                        "raw":       item["raw"],
                        "rewritten": rewritten,
                        "timestamp": datetime.now(KST).isoformat()
                    })
                    # print(f"[DEBUG] âœ… finalì— ì¶”ê°€ë¨: {item['rewritten'][:50]}...")
                else:
                    # print(f"[DEBUG] âš ï¸ ì¤‘ë³µëœ ë‹µë³€(final)ì— ì¶”ê°€í•˜ì§€ ì•ŠìŒ: {item['rewritten'][:50]}...")
                    pass

            # ê°•ì œ í†µê³¼ í”Œë˜ê·¸ê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ finalì— ì¶”ê°€
            if force and not item.get("ok", False):
                print("âš ï¸ ê°•ì œ í†µê³¼ í”Œë˜ê·¸ë¡œ ì¸í•´ finalì— ì¶”ê°€")
                rewritten = item["rewritten"]
                if not any(f["rewritten"] == rewritten for f in rewrite.get("final", [])):
                    rewrite.setdefault("final", []).append({
                        "raw":       item["raw"],
                        "rewritten": rewritten,
                        "timestamp": datetime.now(KST).isoformat()
                    })
                    # print(f"[DEBUG] âœ… ê°•ì œ í†µê³¼ë¡œ finalì— ì¶”ê°€ë¨: {item['rewritten'][:50]}...")
                else:
                    # print(f"[DEBUG] âš ï¸ ê°•ì œ í†µê³¼ ì¤‘ë³µ(final)ì— ì¶”ê°€í•˜ì§€ ì•ŠìŒ: {item['rewritten'][:50]}...")
                    pass
                item["ok"] = True
                item["judge_notes"].append("ê°•ì œ í†µê³¼ (ì¬ì‹œë„ 3íšŒ ì´ˆê³¼)")

            state.setdefault("decision_log", []).append({
                "step":   "rewrite_judge_agent",
                "result": f"ok={item['ok']}",
                "time":   datetime.now(KST).isoformat(),
                "details":{"notes":item["judge_notes"],"elapsed_sec":round(elapsed,2)}
            })
            print(f"[LangGraph] âœ… íŒì • ê²°ê³¼: ok={item['ok']}")

        except Exception as e:
            print(f"[DEBUG] âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            print(f"[DEBUG] ğŸ” ì›ë³¸ LLM ì‘ë‹µ: {llm_response if 'llm_response' in locals() else 'N/A'}")
            item["ok"]          = False
            item["judge_notes"] = [f"judge error: {e}"]
            state.setdefault("decision_log", []).append({
                "step":"rewrite_judge_agent",
                "result":"error",
                "time":datetime.now(KST).isoformat(),
                "details":{"error":str(e)}
            })

    # ë§ˆì§€ë§‰ í•­ëª©ì´ ok=Trueë©´ ì™„ë£Œ í‘œì‹œ
    if rewrite["items"][-1].get("ok", False):
        rewrite["done"] = True
    return state


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5) Nonverbal í‰ê°€ ì—ì´ì „íŠ¸ (í‘œì •ë§Œ í‰ê°€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def nonverbal_evaluation_agent(state: InterviewState) -> InterviewState:
    """
    ë¹„ì–¸ì–´ì  ìš”ì†Œ(í‘œì •)ë¥¼ í‰ê°€í•˜ëŠ” ì—ì´ì „íŠ¸
    
    Args:
        state (InterviewState): ë©´ì ‘ ìƒíƒœ ê°ì²´
        
    Returns:
        InterviewState: ë¹„ì–¸ì–´ì  í‰ê°€ ê²°ê³¼ê°€ ì¶”ê°€ëœ ìƒíƒœ
        
    ì²˜ë¦¬ ê³¼ì •:
    1. nonverbal_countsì—ì„œ í‘œì • ë°ì´í„° ì¶”ì¶œ
    2. FacialExpression ê°ì²´ë¡œ ë³€í™˜
    3. GPT-4o-minië¡œ í‘œì • íŒ¨í„´ ë¶„ì„
    4. 0.0~1.0 ì ìˆ˜ë¥¼ 15ì  ë§Œì ìœ¼ë¡œ í™˜ì‚°
    5. ê²°ê³¼ë¥¼ state["evaluation"]["results"]["ë¹„ì–¸ì–´ì "]ì— ì €ì¥
    
    Note:
        - í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ìˆ˜ì§‘ëœ smile, neutral, frown, angry íšŸìˆ˜ ê¸°ë°˜
        - ì ì ˆí•œ í‘œì • ë³€í™”ì™€ ì›ƒìŒì€ ê¸ì •ì  í‰ê°€
        - ë°ì´í„° ëˆ„ë½ ì‹œ ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥
    """
    # í‰ê°€ ì‹œì‘ ì‹œê°„ ê¸°ë¡
    evaluation_start_time = datetime.now(KST).timestamp()
    state["_evaluation_start_time"] = evaluation_start_time
    print(f"[â±ï¸] í‰ê°€ ì‹œì‘: {datetime.now(KST).strftime('%Y-%m-%d %H:%M:%S')}")
    
    ts = datetime.now(KST).isoformat()
    try:
        counts = safe_get(state, "nonverbal_counts", {}, context="nonverbal_evaluation_agent")
        print(f"[DEBUG] nonverbal_counts: {counts}")
        # êµ¬ì¡° ì²´í¬
        if not counts or not isinstance(counts, dict):
            print("[WARNING] nonverbal_countsê°€ dictê°€ ì•„ë‹˜ ë˜ëŠ” ë¹„ì–´ìˆìŒ. ë¹„ì–¸ì–´ì  í‰ê°€ë¥¼ ê±´ë„ˆëœ€.")
            state.decision_log.append("Nonverbal data not available for evaluation.")
            return state
        if "expression" not in counts or not isinstance(counts["expression"], dict):
            print("[WARNING] nonverbal_counts['expression']ê°€ dictê°€ ì•„ë‹˜ ë˜ëŠ” ì—†ìŒ. ë¹„ì–¸ì–´ì  í‰ê°€ë¥¼ ê±´ë„ˆëœ€.")
            state.decision_log.append("Nonverbal expression data not available for evaluation.")
            return state
        # expression ë‚´ë¶€ í‚¤ ì²´í¬
        exp = counts["expression"]
        required_keys = ["smile", "neutral", "frown", "angry"]
        for k in required_keys:
            if k not in exp or not isinstance(exp[k], int):
                print(f"[WARNING] nonverbal_counts['expression']ì— {k}ê°€ ì—†ê±°ë‚˜ intê°€ ì•„ë‹˜: {exp}")
        facial = FacialExpression.parse_obj(exp)
        print(f"[DEBUG] facial_expression: {facial}")
        res = await evaluate(facial)
        score = res.get("score", 0)
        analysis = res.get("analysis", "")
        feedback = res.get("feedback", "")
        # print(f"[DEBUG] ë¹„ì–¸ì–´ì  í‰ê°€ ê²°ê³¼(score): {score}, analysis: {analysis}, feedback: {feedback}")
        pts = int(round(score * 15))
        if pts == 0:
            print("[WARNING] ë¹„ì–¸ì–´ì  í‰ê°€ ì ìˆ˜ê°€ 0ì…ë‹ˆë‹¤. í”„ë¡ íŠ¸/ë°ì´í„° ì „ë‹¬/LLM í”„ë¡¬í”„íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        evaluation = safe_get(state, "evaluation", {}, context="nonverbal_evaluation_agent")
        results = safe_get(evaluation, "results", {}, context="nonverbal_evaluation_agent")
        results["ë¹„ì–¸ì–´ì "] = {"score": pts, "reason": analysis or feedback or "í‰ê°€ ì‚¬ìœ ì—†ìŒ"}
        state.setdefault("evaluation", {}).setdefault("results", {})["ë¹„ì–¸ì–´ì "] = {
            "score": pts,
            "reason": analysis or feedback or "í‰ê°€ ì‚¬ìœ ì—†ìŒ"
        }
        state.setdefault("decision_log", []).append({
            "step": "nonverbal_evaluation",
            "result": "success",
            "time": ts,
            "details": {
                "score": pts
            }
        })
        print(f"[DEBUG] nonverbal_evaluation_agent - state['evaluation']['results']['ë¹„ì–¸ì–´ì ']: {state.get('evaluation', {}).get('results', {}).get('ë¹„ì–¸ì–´ì ')}")
    except Exception as e:
        print(f"[ERROR] ë¹„ì–¸ì–´ì  í‰ê°€ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        state.setdefault("decision_log", []).append({
            "step": "nonverbal_evaluation",
            "result": "error",
            "time": ts,
            "details": {"error": str(e)}
        })
    return state

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6) í‰ê°€ ì¬ì‹œë„ ì¡°ê±´: ìµœëŒ€ 3íšŒ â†’ ë‹¨ 1íšŒë§Œ ìˆ˜í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def should_retry_evaluation(state: InterviewState) -> Literal["retry", "continue", "done"]:
    """
    í‰ê°€ ì¬ì‹œë„ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ëŠ” ì¡°ê±´ í•¨ìˆ˜
    
    Args:
        state (InterviewState): ë©´ì ‘ ìƒíƒœ ê°ì²´
        
    Returns:
        Literal["retry", "continue", "done"]: ì¬ì‹œë„, ê³„ì†, ì™„ë£Œ
        
    ì²˜ë¦¬ ë¡œì§:
    - í‰ê°€ ì„±ê³µ ë˜ëŠ” ì¬ì‹œë„ 1íšŒ ë„ë‹¬ ì‹œ "continue"
    - ê·¸ ì™¸ì˜ ê²½ìš° "retry" (ìµœëŒ€ 2ë²ˆ ì‹¤í–‰)
    
    Note:
        - ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ 1íšŒë¡œ ì œí•œ (ë¹„ìš© ì ˆì•½)
        - ì´ 2ë²ˆ ì‹¤í–‰ í›„ ë¬´ì¡°ê±´ ì§„í–‰
        - ë‚´ìš© ê²€ì¦ì€ evaluation_judge_agentì—ì„œ ìˆ˜í–‰
    """
    evaluation = safe_get(state, "evaluation", {}, context="should_retry_evaluation:evaluation")
    retry_count = safe_get(evaluation, "retry_count", 0, context="should_retry_evaluation:retry_count")
    is_ok = safe_get(evaluation, "ok", False, context="should_retry_evaluation:ok")
    
    # print(f"[DEBUG] should_retry_evaluation - retry_count: {retry_count}, is_ok: {is_ok}")
    
    # í‰ê°€ê°€ ì„±ê³µí–ˆê±°ë‚˜ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜(1íšŒ)ì— ë„ë‹¬í•œ ê²½ìš° (ì´ 2ë²ˆ ì‹¤í–‰)
    if is_ok or retry_count >= 1:
        # print(f"[DEBUG] í‰ê°€ ì™„ë£Œ - ok: {is_ok}, retry_count: {retry_count}")
        return "continue"
    
    # ì¬ì‹œë„ í•„ìš”
    # print(f"[DEBUG] í‰ê°€ ì¬ì‹œë„ í•„ìš” - retry_count: {retry_count}")
    return "retry"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7) LLM í‚¤ì›Œë“œ í‰ê°€ ì—ì´ì „íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def evaluation_agent(state: InterviewState) -> InterviewState:
    """
    8ê°œ í‚¤ì›Œë“œ Ã— 3ê°œ ê¸°ì¤€ìœ¼ë¡œ ë©´ì ‘ ë‹µë³€ì„ í‰ê°€í•˜ëŠ” ë©”ì¸ ì—ì´ì „íŠ¸
    
    Args:
        state (InterviewState): ë©´ì ‘ ìƒíƒœ ê°ì²´
        
    Returns:
        InterviewState: í‰ê°€ ê²°ê³¼ê°€ ì¶”ê°€ëœ ìƒíƒœ
        
    ì²˜ë¦¬ ê³¼ì •:
    1. ë¦¬ë¼ì´íŒ…ëœ ë‹µë³€ ë˜ëŠ” STT ì›ë³¸ ë‹µë³€ ì¶”ì¶œ
    2. 8ê°œ í‚¤ì›Œë“œë³„ 3ê°œ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€ ìˆ˜í–‰
    3. ì ìˆ˜, ì‚¬ìœ , ì¸ìš©êµ¬ í¬í•¨í•œ ìƒì„¸ ê²°ê³¼ ìƒì„±
    4. ê¸°ì¡´ ë¹„ì–¸ì–´ì  í‰ê°€ ê²°ê³¼ì™€ ë³‘í•©
    
    í‰ê°€ ì˜ì—­:
    - ì¸ì„±ì  ìš”ì†Œ (90ì ): SUPEX, VWBE, Passionate, Proactive, Professional, People
    - ê¸°ìˆ /ì§ë¬´ (15ì ): ì‹¤ë¬´ ê¸°ìˆ /ì§€ì‹, ë¬¸ì œ í•´ê²° ì ìš©ë ¥, í•™ìŠµ ë°œì „ ê°€ëŠ¥ì„±
    - ë„ë©”ì¸ ì „ë¬¸ì„± (15ì ): ë„ë©”ì¸ ì´í•´ë„, ì‹¤ì œ ì‚¬ë¡€ ì ìš©, ì „ëµì  ì‚¬ê³ ë ¥
    
    Note:
        - GPT-4o-mini ì‚¬ìš©ìœ¼ë¡œ ë¹„ìš© ì ˆì•½
        - ê²°ê³¼ ì •ê·œí™”ë¡œ ë°ì´í„° ì¼ê´€ì„± ë³´ì¥
        - ì¬ì‹œë„ íšŸìˆ˜ ê´€ë¦¬ í¬í•¨
    """
    rewrite = safe_get(state, "rewrite", {}, context="evaluation_agent:rewrite")
    final_items = safe_get(rewrite, "final", [], context="evaluation_agent:rewrite.final")
    print(f"[DEBUG] ğŸ“ evaluation_agent - final_items ê°œìˆ˜: {len(final_items)}")
    if final_items:
        for idx, item in enumerate(final_items):
            print(f"[DEBUG] ğŸ“ final[{idx}]: {item.get('rewritten', '')[:100]}")
        full_answer = "\n".join(item["rewritten"] for item in final_items)
    else:
        print("[DEBUG] âš ï¸ final_itemsê°€ ë¹„ì–´ìˆìŒ. ëª¨ë“  STT ì›ë³¸ ë‹µë³€ì„ í•©ì³ì„œ í‰ê°€")
        stt_segments = state.get("stt", {}).get("segments", [])
        if stt_segments:
            full_answer = "\n".join(seg.get("raw", "ë‹µë³€ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.") for seg in stt_segments)
        else:
            full_answer = "ë‹µë³€ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
    
    print(f"[DEBUG] ğŸ“„ í‰ê°€í•  ë‹µë³€: {full_answer[:100]}...")
    
    # í‰ê°€ ê¸°ì¤€ í‚¤ ëª©ë¡ì„ ê°€ì ¸ì˜´
    all_criteria = {**EVAL_CRITERIA_WITH_ALL_SCORES, **TECHNICAL_EVAL_CRITERIA_WITH_ALL_SCORES, **DOMAIN_EVAL_CRITERIA_WITH_ALL_SCORES}

    # í‰ê°€ ê²°ê³¼ë¥¼ ì •ì œí•˜ëŠ” í•¨ìˆ˜ (quotes í•„ë“œê¹Œì§€ ë³´ì¥)
    def normalize_results(results):
        normalized = {}
        for keyword, criteria in all_criteria.items():
            kw_result = results.get(keyword, {}) if isinstance(results, dict) else {}
            normalized[keyword] = {}
            for crit_name in criteria.keys():
                val = kw_result.get(crit_name) if isinstance(kw_result, dict) else None
                # ë³´ê°•: dictê°€ ì•„ë‹ˆë©´ ë¬´ì¡°ê±´ dictë¡œ ê°ì‹¸ê¸°
                if not isinstance(val, dict):
                    if isinstance(val, int):
                        val = {"score": val, "reason": "í‰ê°€ ì‚¬ìœ ì—†ìŒ", "quotes": []}
                    else:
                        val = {"score": 1, "reason": "í‰ê°€ ì‚¬ìœ ì—†ìŒ", "quotes": []}
                score = val.get("score", 1)
                reason = val.get("reason", "í‰ê°€ ì‚¬ìœ ì—†ìŒ")
                quotes = val.get("quotes", [])
                if not isinstance(quotes, list):
                    quotes = []
                normalized[keyword][crit_name] = {
                    "score": score,
                    "reason": reason,
                    "quotes": quotes
                }
        # ë¹„ì–¸ì–´ì  ìš”ì†Œë„ í•­ìƒ dictë¡œ ë³´ì •
        if "ë¹„ì–¸ì–´ì " in results:
            nonverbal = results["ë¹„ì–¸ì–´ì "]
            if not isinstance(nonverbal, dict):
                if isinstance(nonverbal, int):
                    nonverbal = {"score": nonverbal, "reason": "í‰ê°€ ì‚¬ìœ ì—†ìŒ", "quotes": []}
                else:
                    nonverbal = {"score": 1, "reason": "í‰ê°€ ì‚¬ìœ ì—†ìŒ", "quotes": []}
            if "quotes" not in nonverbal or not isinstance(nonverbal["quotes"], list):
                nonverbal["quotes"] = []
            normalized["ë¹„ì–¸ì–´ì "] = nonverbal
        return normalized

    results = await evaluate_keywords_from_full_answer(full_answer)
    results = normalize_results(results)

    prev_eval = safe_get(state, "evaluation", {}, context="evaluation_agent:evaluation")
    prev_results = prev_eval.get("results", {})
    # ê¸°ì¡´ ë¹„ì–¸ì–´ì  ë“± ê²°ê³¼ì™€ ìƒˆ í‰ê°€ ê²°ê³¼ ë³‘í•©
    merged_results = {**prev_results, **results}
    prev_retry = safe_get(prev_eval, "retry_count", 0, context="evaluation_agent:evaluation.retry_count")
    if "ok" in prev_eval and safe_get(prev_eval, "ok", context="evaluation_agent:evaluation.ok") is False:
        retry_count = prev_retry + 1
    else:
        retry_count = prev_retry

    state["evaluation"] = {
        **prev_eval,
        "done": True,
        "results": merged_results,
        "retry_count": retry_count,
        "ok": False  # íŒì • ì „ì´ë¯€ë¡œ Falseë¡œ ì´ˆê¸°í™”
    }
    state["done"] = True  # íŒŒì´í”„ë¼ì¸ ì „ì²´ ì¢…ë£Œ ì‹ í˜¸ ì¶”ê°€
    ts = datetime.now(KST).isoformat()
    state.setdefault("decision_log", []).append({
        "step": "evaluation_agent",
        "result": "done",
        "time": ts,
        "details": {"retry_count": retry_count}
    })

    return state

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 8) í‰ê°€ ê²€ì¦ ì—ì´ì „íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def evaluation_judge_agent(state: InterviewState) -> InterviewState:
    """
    í‰ê°€ ê²°ê³¼ë¥¼ ê²€ì¦í•˜ëŠ” íŒì • ì—ì´ì „íŠ¸
    
    Args:
        state (InterviewState): ë©´ì ‘ ìƒíƒœ ê°ì²´
        
    Returns:
        InterviewState: ê²€ì¦ ê²°ê³¼ê°€ ì¶”ê°€ëœ ìƒíƒœ
        
    ê²€ì¦ í•­ëª©:
    1. êµ¬ì¡°ì  ê²€ì¦: ê° í‚¤ì›Œë“œë‹¹ 3ê°œ ê¸°ì¤€ ì¡´ì¬ ì—¬ë¶€
    2. ì ìˆ˜ ë²”ìœ„ ê²€ì¦: 1~5ì  ë²”ìœ„ ë‚´ ì ìˆ˜ í™•ì¸
    3. ì´ì  ê²€ì¦: ìµœëŒ€ ì ìˆ˜ ì´ˆê³¼ ì—¬ë¶€ í™•ì¸
    4. ë‚´ìš© ê²€ì¦: GPT-4o-minië¡œ í‰ê°€ ë‚´ìš© íƒ€ë‹¹ì„± ê²€ì¦
    
    Note:
        - ê²€ì¦ ì‹¤íŒ¨ ì‹œì—ë„ ì¬ì‹œë„ ì œí•œìœ¼ë¡œ ì§„í–‰
        - ë‚´ìš© ê²€ì¦ ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ í†µê³¼ ì²˜ë¦¬
        - ëª¨ë“  ê²€ì¦ ê²°ê³¼ë¥¼ decision_logì— ê¸°ë¡
    """
    evaluation = safe_get(state, "evaluation", {}, context="evaluation_judge_agent:evaluation")
    results = safe_get(evaluation, "results", {}, context="evaluation_judge_agent:evaluation.results")
    if not results:
        state.setdefault("decision_log", []).append({
            "step": "evaluation_judge_agent",
            "result": "error",
            "time": datetime.now(KST).isoformat(),
            "details": {"error": "No evaluation results found"}
        })
        print("[judge] No evaluation results found, will stop.")
        state["evaluation"]["ok"] = True  # ë” ì´ìƒ retry/continue ì•ˆ í•˜ë„ë¡ Trueë¡œ ì„¤ì •
        return state

    judge_notes = []
    is_valid = True

    # 1. í•­ëª© ìˆ˜ ê²€ì¦ (ê° í‚¤ì›Œë“œì— 3ê°œ)
    for kw, criteria in results.items():
        if len(criteria) != 3:
            judge_notes.append(f"Keyword '{kw}' has {len(criteria)} criteria (expected 3)")
            is_valid = False

    # 2. ì ìˆ˜ ë²”ìœ„ ê²€ì¦ (1~5)
    for criteria in results.values():
        for data in criteria.values():
            # print(f"[DEBUG] evaluation_judge_agent - data type: {type(data)}, value: {data}")
            if isinstance(data, dict):
                s = data.get("score", 0)
            elif isinstance(data, int):
                s = data
            else:
                s = 0
            if not (1 <= s <= 5):
                judge_notes.append(f"Invalid score {s}")
                is_valid = False

    # 3. ì´ì  ê²€ì¦
    total = 0
    for crit in results.values():
        for c in crit.values():
            if isinstance(c, dict):
                total += c.get("score", 0)
            elif isinstance(c, int):
                total += c
            else:
                total += 0
    max_score = len(results) * 3 * 5
    if total > max_score:
        judge_notes.append(f"Total {total} exceeds max {max_score}")
        is_valid = False

    print(f"[judge] is_valid={is_valid}, judge_notes={judge_notes}, total={total}, max_score={max_score}")
    state["evaluation"]["judge"] = {
        "ok": is_valid,
        "judge_notes": judge_notes,
        "total_score": total,
        "max_score": max_score
    }
    state["evaluation"]["ok"] = is_valid

    # === ë‚´ìš© ê²€ì¦ LLM í˜¸ì¶œ ì¶”ê°€ ===
    try:
        CONTENT_VALIDATION_PROMPT = """
ì‹œìŠ¤í…œ: ë‹¹ì‹ ì€ AI ë©´ì ‘ í‰ê°€ ê²°ê³¼ì˜ ê²€ì¦ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ëŠ” ì§€ì›ìì˜ ë‹µë³€, ê·¸ë¦¬ê³  ê·¸ ë‹µë³€ì— ëŒ€í•œ í‚¤ì›Œë“œë³„ í‰ê°€ ê²°ê³¼ì…ë‹ˆë‹¤.

[ì§€ì›ì ë‹µë³€]
{answer}

[í‰ê°€ ê²°ê³¼]
{evaluation}

[í‰ê°€ ê¸°ì¤€]
{criteria}

í‰ê°€ ê²°ê³¼ë¥¼ ê°„ë‹¨íˆ ê²€ì¦í•˜ê³  ì•„ë˜ í˜•ì‹ì˜ JSONìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.

{{
  "ok": true,
  "judge_notes": ["í‰ê°€ ì™„ë£Œ"]
}}
"""
        final_items = safe_get(state, "rewrite", {}).get("final", [])
        if not final_items:
            # final_itemsê°€ ë¹„ì–´ìˆìœ¼ë©´ raw í…ìŠ¤íŠ¸ ì‚¬ìš©
            stt_segments = state.get("stt", {}).get("segments", [])
            if stt_segments:
                answer = stt_segments[-1].get("raw", "ë‹µë³€ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                answer = "ë‹µë³€ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
        else:
            answer = "\n".join(item["rewritten"] for item in final_items)
            
        evaluation = json.dumps(state.get("evaluation", {}).get("results", {}), ensure_ascii=False)
        criteria = json.dumps({
            **EVAL_CRITERIA_WITH_ALL_SCORES,
            **TECHNICAL_EVAL_CRITERIA_WITH_ALL_SCORES,
            **DOMAIN_EVAL_CRITERIA_WITH_ALL_SCORES
        }, ensure_ascii=False)

        prompt = CONTENT_VALIDATION_PROMPT.format(
            answer=answer,
            evaluation=evaluation,
            criteria=criteria
        )

        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=1024
        )
        
        llm_response = response.choices[0].message.content.strip()
        # print(f"[DEBUG] ğŸ¤– ë‚´ìš© ê²€ì¦ LLM ì‘ë‹µ: {llm_response}")
        
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
        if llm_response.startswith("```json"):
            llm_response = llm_response[7:]  # "```json" ì œê±°
        if llm_response.startswith("```"):
            llm_response = llm_response[3:]   # "```" ì œê±°
        if llm_response.endswith("```"):
            llm_response = llm_response[:-3]  # ëì˜ "```" ì œê±°
        
        llm_response = llm_response.strip()
        # print(f"[DEBUG] ğŸ”§ ì •ë¦¬ëœ ë‚´ìš© ê²€ì¦ JSON: {llm_response}")
        
        if not llm_response:
            raise ValueError("LLM ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
        result = json.loads(llm_response)
        state["evaluation"]["content_judge"] = result
        print(f"[LangGraph] âœ… ë‚´ìš© ê²€ì¦ ê²°ê³¼: ok={result.get('ok')}, notes={result.get('judge_notes')}")
    except Exception as e:
        print(f"[DEBUG] âŒ ë‚´ìš© ê²€ì¦ ì˜¤ë¥˜: {e}")
        # print(f"[DEBUG] ğŸ” LLM ì‘ë‹µ: {llm_response if 'llm_response' in locals() else 'N/A'}")
        state["evaluation"]["content_judge"] = {
            "ok": True,  # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ì ìœ¼ë¡œ í†µê³¼
            "judge_notes": [f"content judge error: {e}"]
        }
        print(f"[LangGraph] âŒ ë‚´ìš© ê²€ì¦ ì˜¤ë¥˜: {e}")

    ts = datetime.now(KST).isoformat()
    state.setdefault("decision_log", []).append({
        "step": "evaluation_judge_agent",
        "result": f"ok={is_valid}",
        "time": ts,
        "details": {
            "total_score": total,
            "max_score": max_score,
            "notes": judge_notes
        }
    })
    
    return state

def calculate_area_scores(evaluation_results, nonverbal_score):
    """
    ì˜ì—­ë³„ ì ìˆ˜ ê³„ì‚° ë° 100ì  ë§Œì  í™˜ì‚° í•¨ìˆ˜
    
    Args:
        evaluation_results (dict): í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        nonverbal_score (int): ë¹„ì–¸ì–´ì  ì ìˆ˜ (15ì  ë§Œì )
        
    Returns:
        tuple: (weights, personality_score_scaled, job_domain_score_scaled, nonverbal_score_scaled)
        
    ê³„ì‚° ë°©ì‹:
    - ì¸ì„±ì  ìš”ì†Œ (90ì  ë§Œì ): SUPEX, VWBE, Passionate, Proactive, Professional, People
    - ì§ë¬´Â·ë„ë©”ì¸ (30ì  ë§Œì ): "ê¸°ìˆ /ì§ë¬´", "ë„ë©”ì¸ ì „ë¬¸ì„±"  
    - ë¹„ì–¸ì–´ì  ìš”ì†Œ (15ì  ë§Œì ): í‘œì • ë¶„ì„ ì ìˆ˜
    
    í™˜ì‚° ë¹„ìœ¨:
    - ì¸ì„±ì  ìš”ì†Œ: 45% (90ì  â†’ 45ì )
    - ì§ë¬´Â·ë„ë©”ì¸: 45% (30ì  â†’ 45ì )
    - ë¹„ì–¸ì–´ì  ìš”ì†Œ: 10% (15ì  â†’ 10ì )
    """
    personality_keywords = ["SUPEX", "VWBE", "Passionate", "Proactive", "Professional", "People"]
    job_domain_keywords = ["ê¸°ìˆ /ì§ë¬´", "ë„ë©”ì¸ ì „ë¬¸ì„±"]
    
    # ì–¸ì–´ì  ìš”ì†Œ ì´ì 
    personality_score = 0
    for keyword in personality_keywords:
        for criterion in evaluation_results.get(keyword, {}).values():
            personality_score += criterion.get("score", 0)
    # print(f"[DEBUG] ì¸ì„±ì  ìš”ì†Œ ì´ì : {personality_score} (max 90)")
    
    # ì§ë¬´Â·ë„ë©”ì¸ ì´ì 
    job_domain_score = 0
    for keyword in job_domain_keywords:
        for criterion in evaluation_results.get(keyword, {}).values():
            job_domain_score += criterion.get("score", 0)
    # print(f"[DEBUG] ì§ë¬´Â·ë„ë©”ì¸ ì´ì : {job_domain_score} (max 30)")
    
    # ë¹„ì–¸ì–´ì  ìš”ì†Œ
    # print(f"[DEBUG] ë¹„ì–¸ì–´ì  ìš”ì†Œ ì›ì ìˆ˜: {nonverbal_score} (max 15)")
    max_personality = 90
    max_job_domain = 30
    max_nonverbal = 15
    
    # 100ì  ë§Œì  í™˜ì‚° ì ìˆ˜ ê³„ì‚°
    personality_score_scaled = round((personality_score / max_personality) * 45, 1) if max_personality else 0
    job_domain_score_scaled = round((job_domain_score / max_job_domain) * 45, 1) if max_job_domain else 0
    nonverbal_score_scaled = round((nonverbal_score / max_nonverbal) * 10, 1) if max_nonverbal else 0
    
    # ë¹„ì¤‘ (ê³ ì •ê°’)
    weights = {
        "ì¸ì„±ì  ìš”ì†Œ": 45.0,
        "ì§ë¬´Â·ë„ë©”ì¸": 45.0,
        "ë¹„ì–¸ì–´ì  ìš”ì†Œ": 10.0
    }
    
    # print(f"[DEBUG] í™˜ì‚° ì ìˆ˜: ì¸ì„±ì ={personality_score_scaled}, ì§ë¬´Â·ë„ë©”ì¸={job_domain_score_scaled}, ë¹„ì–¸ì–´ì ={nonverbal_score_scaled}")
    return weights, personality_score_scaled, job_domain_score_scaled, nonverbal_score_scaled

EVAL_REASON_SUMMARY_PROMPT = """
ì•„ë˜ëŠ” ì§€ì›ìì˜ ì „ì²´ ë‹µë³€ê³¼ ê° í‰ê°€ í‚¤ì›Œë“œë³„ í‰ê°€ ì‚¬ìœ (reason)ì…ë‹ˆë‹¤.

[ì§€ì›ì ë‹µë³€]
{answer}

[í‰ê°€ ì‚¬ìœ ]
{all_reasons}

ì´ ë‘ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬, ì§€ì›ìê°€ ì´ë ‡ê²Œ ì ìˆ˜ë¥¼ ì–»ê²Œ ëœ ì´ìœ ë¥¼ 8ì¤„ ì´ë‚´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ìš”ì•½í•´ ì£¼ì„¸ìš”.
- í‰ê°€ ê·¼ê±°ì™€ ì§€ì›ìì˜ í•µì‹¬ ë‹µë³€ ë‚´ìš©ì´ ëª¨ë‘ í¬í•¨ë˜ë„ë¡ í•˜ì„¸ìš”.
- ê° ì¤„ì€ ê°„ê²°í•˜ê³  í•µì‹¬ì ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
- ì¤‘ë³µë˜ëŠ” ë‚´ìš©ì€ í•©ì¹˜ê³ , ì¤‘ìš”í•œ íŠ¹ì§•/ê°•ì /ë³´ì™„ì ì´ ë“œëŸ¬ë‚˜ë„ë¡ í•´ ì£¼ì„¸ìš”.
- ë°˜ë“œì‹œ 8ì¤„ ì´ë‚´ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”.
"""

async def score_summary_agent(state):
    """
    í‰ê°€ ê²€ì¦ í›„ ìµœì¢… ì ìˆ˜ í™˜ì‚° ë° ìš”ì•½ì„ ë‹´ë‹¹í•˜ëŠ” ì—ì´ì „íŠ¸
    
    Args:
        state (InterviewState): ë©´ì ‘ ìƒíƒœ ê°ì²´
        
    Returns:
        InterviewState: ìµœì¢… ìš”ì•½ì´ ì¶”ê°€ëœ ìƒíƒœ
        
    ì²˜ë¦¬ ê³¼ì •:
    1. ì˜ì—­ë³„ ì ìˆ˜ ê³„ì‚° ë° 100ì  ë§Œì  í™˜ì‚°
    2. ì§€ì›ì ë‹µë³€ê³¼ í‰ê°€ ì‚¬ìœ ë¥¼ GPT-4oë¡œ ì¢…í•© ìš”ì•½
    3. í‰ê°€ ì†Œìš”ì‹œê°„ ê³„ì‚° ë° ê¸°ë¡
    4. done í”Œë˜ê·¸ ì„¤ì •ìœ¼ë¡œ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ
    
    ìµœì¢… ê²°ê³¼:
    - ì¸ì„±ì  ìš”ì†Œ: 45% (90ì  â†’ 45ì )
    - ì§ë¬´/ë„ë©”ì¸: 45% (30ì  â†’ 45ì )  
    - ë¹„ì–¸ì–´ì  ìš”ì†Œ: 10% (15ì  â†’ 10ì )
    - ì´ì : 100ì  ë§Œì 
    
    Note:
        - GPT-4o ì‚¬ìš©ìœ¼ë¡œ ê³ í’ˆì§ˆ ìš”ì•½ ìƒì„±
        - í‰ê°€ ì†Œìš”ì‹œê°„ ì¶”ì  ë° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        - ëª¨ë“  ê²°ê³¼ë¥¼ state["summary"]ì— ì €ì¥
    """
    evaluation = safe_get(state, "evaluation", {}, context="score_summary_agent:evaluation")
    evaluation_results = safe_get(evaluation, "results", {}, context="score_summary_agent:evaluation.results")
    # print(f"[DEBUG] í‰ê°€ ê²°ê³¼(evaluation_results): {json.dumps(evaluation_results, ensure_ascii=False, indent=2)}")
    nonverbal = evaluation_results.get("ë¹„ì–¸ì–´ì ", {})
    nonverbal_score = nonverbal.get("score", 0)
    nonverbal_reason = nonverbal.get("reason", "í‰ê°€ ì‚¬ìœ ì—†ìŒ")
    # print(f"[DEBUG] ë¹„ì–¸ì–´ì  í‰ê°€: score={nonverbal_score}, reason={nonverbal_reason}")

    # 100ì  ë§Œì  í™˜ì‚° ì ìˆ˜ ê³„ì‚°
    weights, personality_score_scaled, job_domain_score_scaled, nonverbal_score_scaled = calculate_area_scores(evaluation_results, nonverbal_score)
    verbal_score = personality_score_scaled + job_domain_score_scaled
    # print(f"[DEBUG] verbal_score(ì¸ì„±+ì§ë¬´/ë„ë©”ì¸): {verbal_score}")

    # ì „ì²´ í‚¤ì›Œë“œ í‰ê°€ ì‚¬ìœ  ì¢…í•© (SUPEX, VWBE, Passionate, Proactive, Professional, People, ê¸°ìˆ /ì§ë¬´, ë„ë©”ì¸ ì „ë¬¸ì„±)
    all_keywords = [
        "SUPEX", "VWBE", "Passionate", "Proactive", "Professional", "People",
        "ê¸°ìˆ /ì§ë¬´", "ë„ë©”ì¸ ì „ë¬¸ì„±"
    ]
    reasons = []
    for keyword in all_keywords:
        for crit_name, crit in evaluation_results.get(keyword, {}).items():
            reason = crit.get("reason", "")
            if reason:
                reasons.append(f"{keyword} - {crit_name}: {reason}")
            # print(f"[DEBUG] í‰ê°€ ì‚¬ìœ  ì¶”ì¶œ: {keyword} - {crit_name} - {reason}")
    all_reasons = "\n".join(reasons)
    # print(f"[DEBUG] all_reasons(ì „ì²´ í‰ê°€ ì‚¬ìœ ):\n{all_reasons}")

    # ì§€ì›ì ë‹µë³€ ì¶”ì¶œ
    rewrite = safe_get(state, "rewrite", {}, context="score_summary_agent:rewrite")
    final_items = safe_get(rewrite, "final", [], context="score_summary_agent:rewrite.final")
    if final_items:
        answer = "\n".join(item["rewritten"] for item in final_items)
    else:
        stt = safe_get(state, "stt", {}, context="score_summary_agent:stt")
        stt_segments = safe_get(stt, "segments", [], context="score_summary_agent:stt.segments")
        if stt_segments:
            answer = "\n".join(seg.get("raw", "ë‹µë³€ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.") for seg in stt_segments)
        else:
            answer = "ë‹µë³€ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
    # print(f"[DEBUG] ì§€ì›ì ë‹µë³€(answer):\n{answer}")

    # LLM í”„ë¡¬í”„íŠ¸ë¡œ ì¢…í•© ìš”ì•½ ìš”ì²­
    prompt = EVAL_REASON_SUMMARY_PROMPT.format(answer=answer, all_reasons=all_reasons)
    response = openai.chat.completions.create(
        model="gpt-4o",
        messages=[{
            "role": "user",
            "content": prompt
        }]
    )
    verbal_reason = response.choices[0].message.content.strip().splitlines()[:8]
    # print(f"[DEBUG] summary_text(LLM ìš”ì•½): {verbal_reason}")

    # ê° í‚¤ì›Œë“œë³„ ì´ì  ê³„ì‚°
    keyword_scores = {}
    for keyword, criteria in evaluation_results.items():
        if keyword == "ë¹„ì–¸ì–´ì ":
            continue
        total = 0
        for crit in criteria.values():
            if isinstance(crit, dict):
                total += crit.get("score", 0)
            elif isinstance(crit, int):
                total += crit
        keyword_scores[keyword] = total

    # stateì— ì €ì¥
    state["summary"] = {
        "weights": weights,
        "personality_score": personality_score_scaled,
        "job_domain_score": job_domain_score_scaled,
        "verbal_score": verbal_score,
        "verbal_reason": verbal_reason,
        "nonverbal_score": nonverbal_score_scaled,
        "nonverbal_reason": nonverbal_reason,
        "keyword_scores": keyword_scores,
        "total_score": round(verbal_score + nonverbal_score_scaled, 1)
    }
    print(f"[LangGraph] âœ… ì˜ì—­ë³„ ì ìˆ˜/ìš”ì•½ ì €ì¥: {json.dumps(state['summary'], ensure_ascii=False, indent=2)}")

    # í‰ê°€ ì†Œìš”ì‹œê°„ ê³„ì‚° ë° ì¶œë ¥
    start_time = state.get("_evaluation_start_time")
    if start_time:
        end_time = datetime.now(KST).timestamp()
        total_elapsed = end_time - start_time
        print(f"[â±ï¸] í‰ê°€ ì™„ë£Œ: {datetime.now(KST).strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"[â±ï¸] í‰ê°€ ì†Œìš”ì‹œê°„: {total_elapsed:.2f}ì´ˆ (í‰ê°€ ì‹œì‘ â†’ ì™„ë£Œ)")
        
        # decision_logì—ë„ ê¸°ë¡
        state.setdefault("decision_log", []).append({
            "step": "evaluation_complete",
            "result": "success",
            "time": datetime.now(KST).isoformat(),
            "details": {
                "evaluation_elapsed_seconds": round(total_elapsed, 2),
                "start_time": datetime.fromtimestamp(start_time, KST).isoformat(),
                "end_time": datetime.now(KST).isoformat()
            }
        })
        
        # summaryì—ë„ ì†Œìš”ì‹œê°„ ì •ë³´ ì¶”ê°€
        state["summary"]["evaluation_duration"] = {
            "total_seconds": round(total_elapsed, 2),
            "start_time": datetime.fromtimestamp(start_time, KST).isoformat(),
            "end_time": datetime.now(KST).isoformat()
        }
    else:
        print("[â±ï¸] í‰ê°€ ì‹œì‘ ì‹œê°„ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ - done í”Œë˜ê·¸ ì„¤ì •
    state["done"] = True
    print(f"[LangGraph] âœ… ëª¨ë“  í‰ê°€ ì™„ë£Œ - done í”Œë˜ê·¸ ì„¤ì •")

    return state




# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Excel Node: ì§€ì›ì IDë¡œ ì´ë¦„ ì¡°íšŒ í›„ ì—‘ì…€ ìƒì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# async def excel_node(state: InterviewState) -> InterviewState:
#     import os
#     from datetime import datetime

#     try:
#         applicant_id = safe_get(state, "interviewee_id", context="excel_node:applicant_id")
#         rewrite = safe_get(state, "rewrite", {}, context="excel_node:rewrite")
#         rewrite_final = safe_get(rewrite, "final", [], context="excel_node:rewrite.final")
#         evaluation = safe_get(state, "evaluation", {}, context="excel_node:evaluation")
#         judge = safe_get(evaluation, "judge", {}, context="excel_node:evaluation.judge")
#         total_score = safe_get(judge, "total_score", context="excel_node:evaluation.judge.total_score")

#         # 1. ì§€ì›ì ì •ë³´ ì¡°íšŒ
#         SPRINGBOOT_BASE_URL = os.getenv("SPRING_API_URL", "http://localhost:8080/api/v1")
#         applicant_name = None
#         interviewers = None
#         room_no = None
#         scheduled_at = None
#         async with httpx.AsyncClient() as client:
#             resp = await client.get(f"{SPRINGBOOT_BASE_URL}/interviews/simple")
#             print(f"[DEBUG] /interviews/simple status: {resp.status_code}")
#             print(f"[DEBUG] /interviews/simple response: {resp.text}")
            
#             # ì‘ë‹µ ìƒíƒœ í™•ì¸
#             if resp.status_code != 200:
#                 print(f"[ERROR] /interviews/simple API í˜¸ì¶œ ì‹¤íŒ¨: {resp.status_code} - {resp.text}")
#                 data = []
#             else:
#                 try:
#                     data = safe_get(resp.json(), "data", [], context="excel_node:resp.data")
#                 except Exception as e:
#                     print(f"[ERROR] /interviews/simple JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
#                     print(f"[ERROR] ì‘ë‹µ ë‚´ìš©: {resp.text}")
#                     data = []
        
#         if not isinstance(data, list):
#             print(f"[ERROR] /interviews/simple dataê°€ listê°€ ì•„ë‹˜! ì‹¤ì œ íƒ€ì…: {type(data)}, ê°’: {data}")
#             data = []

#         for item in data:
#             if not isinstance(item, dict):
#                 print(f"[ERROR] /interviews/simple itemì´ dictê°€ ì•„ë‹˜! ì‹¤ì œ íƒ€ì…: {type(item)}, ê°’: {item}")
#                 continue
#             if safe_get(item, "intervieweeId", context="excel_node:item.intervieweeId") == applicant_id:
#                 applicant_name = item["name"]
#                 interviewers = item.get("interviewers", "")
#                 room_no = item.get("roomNo", "")
#                 scheduled = item.get("scheduledAt", [])
#                 if scheduled and len(scheduled) >= 5:
#                     scheduled_at = f"{scheduled[0]:04d}-{scheduled[1]:02d}-{scheduled[2]:02d} {scheduled[3]:02d}:{scheduled[4]:02d}"
#                 break

#         if applicant_name is None:
#             raise ValueError(f"ì§€ì›ì ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. applicant_id={applicant_id}")

#         # 2. ë‹µë³€ í•©ì¹˜ê¸°
#         all_answers = "\n".join([item["rewritten"] for item in rewrite_final])

#         # 3. ì—‘ì…€ ìƒì„±
#         wb = openpyxl.Workbook()
#         ws = wb.active
#         ws.title = "ë©´ì ‘ ê²°ê³¼"
#         ws.append(["ì§€ì›ìID", "ì´ë¦„", "ë©´ì ‘ê´€", "ë©´ì ‘ì‹¤", "ë©´ì ‘ì¼ì‹œ", "ë‹µë³€(ëª¨ë‘)", "ì´ì "])
#         ws.append([applicant_id, applicant_name, interviewers, room_no, scheduled_at, all_answers, total_score])

#         out_dir = os.getenv("RESULT_DIR", "./result")
#         os.makedirs(out_dir, exist_ok=True)
#         ts = datetime.now(KST).strftime("%Y%m%d%H%M%S")
#         excel_path = f"{out_dir}/{applicant_id}_result_{ts}.xlsx"
#         wb.save(excel_path)
#         print(f"[LangGraph] âœ… Excel ìƒì„± ì™„ë£Œ: {excel_path}")

#         state.setdefault("report", {}).setdefault("excel", {})["path"] = excel_path
#         state.setdefault("decision_log", []).append({
#             "step": "excel_node",
#             "result": "generated",
#             "time": datetime.now(KST).isoformat(),
#             "details": {"path": excel_path}
#         })
#     except Exception as e:
#         print(f"[LangGraph] âŒ Excel ìƒì„± ì‹¤íŒ¨: {e}")
#         state.setdefault("report", {}).setdefault("excel", {})["error"] = str(e)
#     return state

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ—ï¸ íŒŒì´í”„ë¼ì¸ ê·¸ë˜í”„ êµ¬ì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# 1) STT â†’ ë¦¬ë¼ì´íŒ… íŒŒì´í”„ë¼ì¸
interview_builder = StateGraph(InterviewState)
interview_builder.add_node("stt_node", stt_node)
interview_builder.add_node("rewrite_agent", rewrite_agent)
interview_builder.add_node("rewrite_judge_agent", rewrite_judge_agent)
interview_builder.set_entry_point("stt_node")
interview_builder.add_edge("stt_node", "rewrite_agent")
interview_builder.add_edge("rewrite_agent", "rewrite_judge_agent")
interview_builder.add_conditional_edges(
    "rewrite_judge_agent", should_retry_rewrite,
    {"retry":"rewrite_agent", "done":"__end__"}
)
interview_flow_executor = interview_builder.compile()

# 2) í‰ê°€ â†’ ìš”ì•½ íŒŒì´í”„ë¼ì¸
final_builder = StateGraph(InterviewState)
final_builder.add_node("nonverbal_eval", nonverbal_evaluation_agent)
final_builder.add_node("evaluation_agent", evaluation_agent)
final_builder.add_node("evaluation_judge_agent", evaluation_judge_agent)
final_builder.add_node("score_summary_agent", score_summary_agent)
# final_builder.add_node("excel_node", excel_node)  # Excel ìƒì„± ë…¸ë“œ (í˜„ì¬ ë¹„í™œì„±í™”)
final_builder.set_entry_point("nonverbal_eval")
final_builder.add_edge("nonverbal_eval", "evaluation_agent")
final_builder.add_edge("evaluation_agent", "evaluation_judge_agent")
# final_builder.add_edge("pdf_node", "excel_node")
final_builder.add_conditional_edges(
    "evaluation_judge_agent", should_retry_evaluation,
    {"retry":"evaluation_agent", "continue":"score_summary_agent", "done":"__end__"}
)
# final_builder.add_channel("decision_log", LastValue())
final_flow_executor = final_builder.compile()

"""
íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ íë¦„:

1. interview_flow_executor (STT â†’ ë¦¬ë¼ì´íŒ…):
   stt_node â†’ rewrite_agent â†’ rewrite_judge_agent â†’ (ì¬ì‹œë„ ì—†ìŒ) â†’ ì™„ë£Œ

2. final_flow_executor (í‰ê°€ â†’ ìš”ì•½):
   nonverbal_eval â†’ evaluation_agent â†’ evaluation_judge_agent â†’ (ì¬ì‹œë„ ìµœëŒ€ 1íšŒ) â†’ score_summary_agent â†’ ì™„ë£Œ

ì „ì²´ íë¦„:
WebM ì˜¤ë””ì˜¤ â†’ STT â†’ ë¦¬ë¼ì´íŒ… â†’ ë¹„ì–¸ì–´ì  í‰ê°€ â†’ ì–¸ì–´ì  í‰ê°€ â†’ ê²€ì¦ â†’ ìš”ì•½ â†’ ì™„ë£Œ
"""