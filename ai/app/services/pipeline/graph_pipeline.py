# app/services/pipeline/graph_pipeline.py

from langgraph.graph import StateGraph
from datetime import datetime
from typing import Literal, Dict, Any
import os
import json
import openai

from app.services.interview.stt_service import transcribe_audio_file
from app.services.interview.rewrite_service import rewrite_answer
from app.services.interview.evaluation_service import evaluate_keywords_from_full_answer
from app.services.interview.report_service import create_radar_chart, generate_pdf
from app.schemas.nonverbal import Posture, FacialExpression, NonverbalData
from app.services.interview.nonverbal_service import evaluate
from app.schemas.state import InterviewState
from langgraph.channels import LastValue, BinaryOperatorAggregate
from app.constants.evaluation_constants_full_all import (
    EVAL_CRITERIA_WITH_ALL_SCORES,
    TECHNICAL_EVAL_CRITERIA_WITH_ALL_SCORES,
    DOMAIN_EVAL_CRITERIA_WITH_ALL_SCORES
)

# ë¦¬ë¼ì´íŒ… ê²€ì¦ìš© í”„ë¡¬í”„íŠ¸
JUDGE_PROMPT = """
ì‹œìŠ¤í…œ: ë‹¹ì‹ ì€ í…ìŠ¤íŠ¸ ë¦¬ë¼ì´íŒ… í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì›ë³¸: "{raw}"
ë¦¬ë¼ì´íŒ…: "{rewritten}"
1) ì˜ë¯¸ ë³´ì¡´
2) ê³¼ì‰ ì¶•ì•½/í™•ì¥
3) ì˜¤íƒˆì/ë¬¸ë§¥ ì˜¤ë¥˜
ìœ„ ê¸°ì¤€ì— ë”°ë¼ JSON í˜•ì‹ìœ¼ë¡œ ok(bool)ì™€ judge_notes(list)ë¥¼ ë°˜í™˜í•˜ì„¸ìš”.
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) STT ë…¸ë“œ: audio_path â†’ raw text
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def stt_node(state: InterviewState) -> InterviewState:
    print("[LangGraph] ğŸ§  stt_node ì§„ì…")
    audio_path = state.get("audio_path")
    raw = transcribe_audio_file(audio_path)
    ts = datetime.now().isoformat()
    state.setdefault("stt", {"done": False, "segments": []})
    state["stt"]["segments"].append({"raw": raw, "timestamp": ts})
    print(f"[LangGraph] âœ… STT ì™„ë£Œ: {raw[:30]}...")
    state.setdefault("decision_log", []).append({
        "step": "stt_node",
        "result": "success",
        "time": ts,
        "details": {"segment_preview": raw[:30]}
    })
    return state

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2) Rewrite ì—ì´ì „íŠ¸: raw â†’ rewritten
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def rewrite_agent(state: InterviewState) -> InterviewState:
    print("[LangGraph] âœï¸ rewrite_agent ì§„ì…")
    raw = state["stt"]["segments"][-1]["raw"]
    rewritten, _ = await rewrite_answer(raw)
    item = {"raw": raw, "rewritten": rewritten}

    prev = state.get("rewrite", {})
    prev_retry = prev.get("retry_count", 0)
    prev_force = prev.get("force_ok", False)
    prev_final = prev.get("final", [])

    # retry_countê°€ 3 ì´ìƒì´ë©´ ë” ì´ìƒ ì¦ê°€ì‹œí‚¤ì§€ ì•ŠìŒ
    if prev_retry >= 3:
        retry_count = prev_retry
    elif prev.get("items") and "ok" in prev["items"][0] and not prev["items"][0]["ok"]:
        retry_count = prev_retry + 1
    else:
        retry_count = prev_retry

    state["rewrite"] = {
        "items":       [item],
        "retry_count": retry_count,
        "force_ok":    prev_force,
        "final":       prev_final,
        "done":        False
    }

    print(f"[LangGraph] âœ… rewrite ê²°ê³¼: {rewritten[:30]}... (retry_count={retry_count})")
    ts = datetime.now().isoformat()
    state.setdefault("decision_log", []).append({
        "step":   "rewrite_agent",
        "result": "processing",
        "time":   ts,
        "details": {"raw_preview": raw[:30], "retry_count": retry_count}
    })
    return state

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3) Rewrite ì¬ì‹œë„ ì¡°ê±´: ìµœëŒ€ 3íšŒ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def should_retry_rewrite(state: InterviewState) -> Literal["retry", "done"]:
    rw    = state.get("rewrite", {})
    items = rw.get("items", [])
    retry = rw.get("retry_count", 0)

    if not items:
        return "retry"

    last = items[-1]
    print(f"ğŸ§ª retry_count={retry}, ok={last.get('ok', 'not_judged')}")

    # ì•„ì§ íŒì •ë˜ì§€ ì•Šì€ ê²½ìš° (ok í‚¤ê°€ ì—†ìŒ)
    if "ok" not in last:
        print(f"ğŸ§ª ì•„ì§ íŒì •ë˜ì§€ ì•ŠìŒ, done")
        return "done"

    # ì„±ê³µí•œ ê²½ìš°
    if last.get("ok", False):
        print(f"ğŸ§ª ì„±ê³µ, done")
        return "done"

    # ì‹¤íŒ¨í•œ ê²½ìš°, ì¬ì‹œë„ ì—¬ë¶€ ê²°ì •
    if retry < 3:
        print(f"ğŸ” Rewrite ì¬ì‹œë„: {retry + 1}íšŒì°¨")
        return "retry"

    # ìµœëŒ€ ì¬ì‹œë„ ë„ë‹¬
    print("ğŸ›‘ ìµœëŒ€ ì¬ì‹œë„ ë„ë‹¬. rewrite ê°•ì œ í†µê³¼ ì˜ˆì •")
    return "done"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4) Rewrite ê²€ì¦ ì—ì´ì „íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def rewrite_judge_agent(state: InterviewState) -> InterviewState:
    print("[LangGraph] ğŸ§ª rewrite_judge_agent ì§„ì…")
    rewrite = state.get("rewrite", {})
    items   = rewrite.get("items", [])
    force   = rewrite.get("force_ok", False)

    if not items:
        state.setdefault("decision_log", []).append({
            "step":   "rewrite_judge_agent",
            "result": "error",
            "time":   datetime.now().isoformat(),
            "details":{"error":"No rewrite items found"}
        })
        return state

    for item in items:
        if "ok" in item:
            continue

        prompt = JUDGE_PROMPT.format(raw=item["raw"], rewritten=item["rewritten"])
        try:
            start = datetime.now().timestamp()
            resp  = openai.ChatCompletion.create(
                model="gpt-4o",
                messages=[{"role":"user","content":prompt}],
                temperature=0, max_tokens=512
            )
            elapsed = datetime.now().timestamp() - start
            result  = json.loads(resp.choices[0].message.content.strip())

            item["ok"]          = result.get("ok", False)
            item["judge_notes"] = result.get("judge_notes", [])

            # ê°•ì œ í†µê³¼ í”Œë˜ê·¸ ì²˜ë¦¬
            if not item["ok"] and force:
                print("âš ï¸ rewrite ì‹¤íŒ¨ í•­ëª© ê°•ì œ ok ì²˜ë¦¬ë¨")
                item["ok"] = True
                item["judge_notes"].append("ìë™ í†µê³¼ (ì¬ì‹œë„ 3íšŒ ì´ˆê³¼)")

            if item["ok"]:
                rewrite.setdefault("final", []).append({
                    "raw":       item["raw"],
                    "rewritten": item["rewritten"],
                    "timestamp": datetime.now().isoformat()
                })

            state.setdefault("decision_log", []).append({
                "step":   "rewrite_judge_agent",
                "result": f"ok={item['ok']}",
                "time":   datetime.now().isoformat(),
                "details":{"notes":item["judge_notes"],"elapsed_sec":round(elapsed,2)}
            })
            print(f"[LangGraph] âœ… íŒì • ê²°ê³¼: ok={item['ok']}")

        except Exception as e:
            item["ok"]          = False
            item["judge_notes"] = [f"judge error: {e}"]
            state.setdefault("decision_log", []).append({
                "step":"rewrite_judge_agent",
                "result":"error",
                "time":datetime.now().isoformat(),
                "details":{"error":str(e)}
            })

    # ë§ˆì§€ë§‰ í•­ëª©ì´ ok=Trueë©´ ì™„ë£Œ í‘œì‹œ
    if rewrite["items"][-1].get("ok", False):
        rewrite["done"] = True

    return state


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5) Nonverbal í‰ê°€ ì—ì´ì „íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def nonverbal_evaluation_agent(state: InterviewState) -> InterviewState:
    ts = datetime.now().isoformat()
    try:
        counts = state.get("nonverbal_counts", {})
        if not counts:
            state.decision_log.append("Nonverbal data not available for evaluation.")
            return state

        posture = Posture.parse_obj(counts["posture"])
        facial  = FacialExpression.parse_obj(counts["facial_expression"])
        gaze    = counts.get("gaze", 0)
        gesture = counts.get("gesture", 0)

        nv = NonverbalData(
            interviewee_id=state["interviewee_id"],
            posture=posture,
            facial_expression=facial,
            gaze=gaze,
            gesture=gesture
        )
        nv_score = await evaluate(nv)

        pts = int(round(nv_score.overall_score * 15))
        reason = nv_score.detailed_analysis or nv_score.feedback.get("ì¢…í•©", "")

        state.setdefault("evaluation", {}).setdefault("results", {})["ë¹„ì–¸ì–´ì "] = {
            "score": pts,
            "reason": reason
        }
        state.setdefault("decision_log", []).append({
            "step": "nonverbal_evaluation",
            "result": "success",
            "time": ts,
            "details": {
                "score": pts,
                "feedback": nv_score.feedback,
                "analysis": nv_score.detailed_analysis,
                "raw_llm_responses": {
                    "posture": nv_score.posture_raw_llm_response,
                    "facial": nv_score.facial_raw_llm_response,
                    "overall": nv_score.overall_raw_llm_response
                }
            }
        })

    except Exception as e:
        state.setdefault("decision_log", []).append({
            "step": "nonverbal_evaluation",
            "result": "error",
            "time": ts,
            "details": {"error": str(e)}
        })
    return state

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6) í‰ê°€ ì¬ì‹œë„ ì¡°ê±´: ìµœëŒ€ 3íšŒ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def should_retry_evaluation(state: InterviewState) -> Literal["retry", "continue"]:
    eval_info = state.get("evaluation", {})
    retry = eval_info.get("retry_count", 0)

    # ì•„ì§ íŒì • ì „ì´ë©´ continue
    if "ok" not in eval_info:
        print("[should_retry_evaluation] Not judged yet, continue")
        return "continue"

    # íŒì • ì‹¤íŒ¨ + ì¬ì‹œë„ íšŸìˆ˜ ë‚¨ì•˜ìœ¼ë©´ retry
    if not eval_info.get("ok", False) and retry < 3:
        print(f"[should_retry_evaluation] Will retry. retry_count={retry + 1}")
        return "retry"

    print(f"[should_retry_evaluation] Continue to next step. ok={eval_info.get('ok')}, retry_count={retry}")
    return "continue"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7) LLM í‚¤ì›Œë“œ í‰ê°€ ì—ì´ì „íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def evaluation_agent(state: InterviewState) -> InterviewState:
    rewritten_items = state.get("rewrite", {}).get("items", [])
    full_answer = "\n".join(item["rewritten"] for item in rewritten_items)
    results = await evaluate_keywords_from_full_answer(full_answer)

    prev_eval = state.get("evaluation", {})
    prev_retry = prev_eval.get("retry_count", 0)
    # ì´ì „ íŒì •ì´ ì‹¤íŒ¨(ok=False)ì˜€ì„ ë•Œë§Œ retry_count ì¦ê°€
    if "ok" in prev_eval and prev_eval.get("ok") is False:
        retry_count = prev_retry + 1
    else:
        retry_count = prev_retry

    state["evaluation"] = {
        "done": True,
        "results": results,
        "retry_count": retry_count,
        "ok": False  # íŒì • ì „ì´ë¯€ë¡œ Falseë¡œ ì´ˆê¸°í™”
    }
    ts = datetime.now().isoformat()
    state.setdefault("decision_log", []).append({
        "step": "evaluation_agent",
        "result": "done",
        "time": ts,
        "details": {"retry_count": retry_count}
    })
    return state

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 8) í‰ê°€ ê²€ì¦ ì—ì´ì „íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def evaluation_judge_agent(state: InterviewState) -> InterviewState:
    results = state.get("evaluation", {}).get("results", {})
    if not results:
        state.setdefault("decision_log", []).append({
            "step": "evaluation_judge_agent",
            "result": "error",
            "time": datetime.now().isoformat(),
            "details": {"error": "No evaluation results found"}
        })
        print("[judge] No evaluation results found, will retry.")
        state["evaluation"]["ok"] = False  # ëª…ì‹œì ìœ¼ë¡œ Falseë¡œ
        return state

    judge_notes = []
    is_valid = True

    # 1. í•­ëª© ìˆ˜ ê²€ì¦ (ê° í‚¤ì›Œë“œì— 3ê°œ)
    for kw, criteria in results.items():
        if len(criteria) != 3:
            judge_notes.append(f"Keyword '{kw}' has {len(criteria)} criteria (expected 3)")
            is_valid = False

    # 2. ì ìˆ˜ ë²”ìœ„ ê²€ì¦ (1~5)
    for criteria in results.values():
        for data in criteria.values():
            s = data.get("score", 0)
            if not (1 <= s <= 5):
                judge_notes.append(f"Invalid score {s}")
                is_valid = False

    # 3. ì´ì  ê²€ì¦
    total = sum(sum(c.get("score", 0) for c in crit.values()) for crit in results.values())
    max_score = len(results) * 3 * 5
    if total > max_score:
        judge_notes.append(f"Total {total} exceeds max {max_score}")
        is_valid = False

    print(f"[judge] is_valid={is_valid}, judge_notes={judge_notes}, total={total}, max_score={max_score}")
    state["evaluation"]["judge"] = {
        "ok": is_valid,
        "judge_notes": judge_notes,
        "total_score": total,
        "max_score": max_score
    }
    state["evaluation"]["ok"] = is_valid

    # === ë‚´ìš© ê²€ì¦ LLM í˜¸ì¶œ ì¶”ê°€ ===
    try:
        CONTENT_VALIDATION_PROMPT = """
ì‹œìŠ¤í…œ: ë‹¹ì‹ ì€ AI ë©´ì ‘ í‰ê°€ ê²°ê³¼ì˜ ê²€ì¦ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ëŠ” ì§€ì›ìì˜ ë‹µë³€, ê·¸ë¦¬ê³  ê·¸ ë‹µë³€ì— ëŒ€í•œ í‚¤ì›Œë“œë³„ í‰ê°€ ê²°ê³¼ì…ë‹ˆë‹¤.

[ì§€ì›ì ë‹µë³€]
{answer}

[í‰ê°€ ê²°ê³¼]
{evaluation}

[í‰ê°€ ê¸°ì¤€]
{criteria}

í‰ê°€ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ì…ë‹ˆë‹¤:
- ê° í‚¤ì›Œë“œë³„ë¡œ 3ê°œì˜ í‰ê°€í•­ëª©ì´ ìˆìŠµë‹ˆë‹¤.
- ê° í‰ê°€í•­ëª©ì—ëŠ” 1~5ì ì˜ ì ìˆ˜ì™€, ê·¸ ì ìˆ˜ì˜ ì‚¬ìœ (ì„¤ëª…)ê°€ ìˆìŠµë‹ˆë‹¤.
- ê° ì ìˆ˜ë³„ë¡œ í‰ê°€ ê¸°ì¤€ì´ ëª…í™•íˆ ì •ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

ì•„ë˜ë¥¼ ê²€ì¦í•˜ì„¸ìš”:
1. ê° í‚¤ì›Œë“œì˜ ê° í‰ê°€í•­ëª©ë³„ ì ìˆ˜ì™€ ì‚¬ìœ ê°€ ì‹¤ì œ ë‹µë³€ ë‚´ìš©ê³¼ ë…¼ë¦¬ì ìœ¼ë¡œ ë§ëŠ”ì§€, ê·¸ë¦¬ê³  í‰ê°€ ê¸°ì¤€ì— ë¶€í•©í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
2. ì ìˆ˜ì™€ ì‚¬ìœ ê°€ ë‹µë³€ ë‚´ìš©ê³¼ ì–´ìš¸ë¦¬ì§€ ì•Šê±°ë‚˜, í‰ê°€ ê¸°ì¤€ì— ë§ì§€ ì•Šìœ¼ë©´ ê·¸ ì´ìœ ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì§€ì í•˜ì„¸ìš”.

ì•„ë˜ í˜•ì‹ì˜ JSONìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.

{{
  "ok": (true ë˜ëŠ” false),
  "judge_notes": [
    "í‚¤ì›Œë“œ 'Proactive'ì˜ 'ì„ ì œì  ë¬¸ì œ ì¸ì‹ê³¼ í–‰ë™' í•­ëª© ì ìˆ˜(5ì )ëŠ” ë‹µë³€ì—ì„œ ì‚¬ì „ ì˜ˆë°©ì  í–‰ë™ì´ êµ¬ì²´ì ìœ¼ë¡œ ë“œëŸ¬ë‚˜ì§€ ì•Šì•„ ê³¼í•˜ê²Œ í‰ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.",
    "í‚¤ì›Œë“œ 'Professional'ì˜ 'ì „ë¬¸ì„± ê¸°ë°˜ ì„±ê³¼ ì°½ì¶œë ¥' í•­ëª© ì‚¬ìœ ê°€ ë‹µë³€ ë‚´ìš©ê³¼ í‰ê°€ ê¸°ì¤€(5ì )ì— ë¶€í•©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
  ]
}}
"""
        rewritten_items = state.get("rewrite", {}).get("items", [])
        answer = "\n".join(item["rewritten"] for item in rewritten_items)
        evaluation = json.dumps(state.get("evaluation", {}).get("results", {}), ensure_ascii=False)
        criteria = json.dumps({
            **EVAL_CRITERIA_WITH_ALL_SCORES,
            **TECHNICAL_EVAL_CRITERIA_WITH_ALL_SCORES,
            **DOMAIN_EVAL_CRITERIA_WITH_ALL_SCORES
        }, ensure_ascii=False)

        prompt = CONTENT_VALIDATION_PROMPT.format(
            answer=answer,
            evaluation=evaluation,
            criteria=criteria
        )

        response = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=1024
        )
        result = json.loads(response.choices[0].message.content.strip())
        state["evaluation"]["content_judge"] = result
        print(f"[LangGraph] âœ… ë‚´ìš© ê²€ì¦ ê²°ê³¼: ok={result.get('ok')}, notes={result.get('judge_notes')}")
    except Exception as e:
        state["evaluation"]["content_judge"] = {
            "ok": False,
            "judge_notes": [f"content judge error: {e}"]
        }
        print(f"[LangGraph] âŒ ë‚´ìš© ê²€ì¦ ì˜¤ë¥˜: {e}")

    ts = datetime.now().isoformat()
    state.setdefault("decision_log", []).append({
        "step": "evaluation_judge_agent",
        "result": f"ok={is_valid}",
        "time": ts,
        "details": {
            "total_score": total,
            "max_score": max_score,
            "notes": judge_notes
        }
    })
    return state


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 9) PDF ìƒì„± ë…¸ë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def pdf_node(state: InterviewState) -> InterviewState:
    """
    ìµœì¢… ë¦¬í¬íŠ¸ ë…¸ë“œ:
    - ì–¸ì–´45% + ë„ë©”ì¸Â·ì§ë¬´45% + ë¹„ì–¸ì–´10% ê°€ì¤‘ì¹˜ë¡œ
    - ì´ì  100ì  ê¸°ì¤€ ê³„ì‚° í›„ PDF ìƒì„±
    """
    from app.services.interview.report_service import create_radar_chart, generate_pdf
    from datetime import datetime

    # rewriteëœ ë‹µë³€
    answers = [i["rewritten"] for i in state["rewrite"]["items"]]
    # í‰ê°€ ê²°ê³¼
    eval_res = state["evaluation"]["results"]
    # ë¹„ì–¸ì–´ ë¶„ë¦¬
    nv = eval_res.pop("ë¹„ì–¸ì–´ì ", {"score":0, "reason":""})
    nv_score  = nv["score"]
    nv_reason = nv["reason"]

    # í‚¤ì›Œë“œë³„ ì›ì ìˆ˜Â·ì‚¬ìœ  ì§‘ê³„
    keyword_results = {
        kw: {
            "score": sum(x.get("score",0) for x in crit.values()),
            "reasons": "\n".join(x.get("reason","") for x in crit.values())
        }
        for kw, crit in eval_res.items()
    }

    # ì¼ë°˜í‚¤ì›Œë“œ vs ë„ë©”ì¸Â·ì§ë¬´ ë¶„ë¦¬
    general_categories_list = ["SUPEX", "V", "WBE", "Passionate", "Proactive", "Professional", "People"]
    job_domain_categories_list = ["ê¸°ìˆ /ì§ë¬´", "ë„ë©”ì¸ ì „ë¬¸ì„±"]

    sum_gen = 0
    for cat in general_categories_list:
        if cat in keyword_results:
            sum_gen += keyword_results[cat]["score"]

    sum_jd = 0
    for cat in job_domain_categories_list:
        if cat in keyword_results:
            sum_jd += keyword_results[cat]["score"]

    # ì˜ì—­ë³„ 100ì  ìŠ¤ì¼€ì¼
    # ìµœëŒ€ ì ìˆ˜
    max_gen_score = len(general_categories_list) * 3 * 5  # 7 categories * 3 criteria/cat * 5 points/criteria = 105
    max_jd_score = len(job_domain_categories_list) * 3 * 5   # 2 categories * 3 criteria/cat * 5 points/criteria = 30
    max_nv_score = 15 # ë¹„ì–¸ì–´ì  ìš”ì†Œì˜ ìµœëŒ€ ì ìˆ˜ëŠ” 15ì ìœ¼ë¡œ ê°€ì •

    area_scores = {
        "ì–¸ì–´ì  ìš”ì†Œ": round(sum_gen / max_gen_score * 100),
        "ì§ë¬´Â·ë„ë©”ì¸":    round(sum_jd  / max_jd_score * 100),
        "ë¹„ì–¸ì–´ì  ìš”ì†Œ": round(nv_score / max_nv_score * 100),
    }
    weights = {"ì–¸ì–´ì  ìš”ì†Œ":"45%", "ì§ë¬´Â·ë„ë©”ì¸":"45%", "ë¹„ì–¸ì–´ì  ìš”ì†Œ":"10%"}

    # ìµœì¢… 100ì  í™˜ì‚°
    total_score = round(
        area_scores["ì–¸ì–´ì  ìš”ì†Œ"]   * 0.45 +
        area_scores["ì§ë¬´Â·ë„ë©”ì¸"]    * 0.45 +
        area_scores["ë¹„ì–¸ì–´ì  ìš”ì†Œ"]  * 0.10
    )

    # ê²½ë¡œ ì¤€ë¹„
    cid = state["interviewee_id"]
    ts  = datetime.now().strftime("%Y%m%d%H%M%S")
    out = "./results"; os.makedirs(out, exist_ok=True)
    chart = f"{out}/{cid}_chart_{ts}.png"
    pdfp  = f"{out}/{cid}_report_{ts}.pdf"

    try:
        create_radar_chart(keyword_results, chart)
        generate_pdf(
            keyword_results=keyword_results,
            chart_path=chart,
            output_path=pdfp,
            interviewee_id=cid,
            answers=answers,
            nonverbal_score=nv_score,
            nonverbal_reason=nv_reason,
            total_score=total_score,
            area_scores=area_scores,
            weights=weights
        )
        state.setdefault("report", {}).setdefault("pdf", {})["generated"] = True
        state["report"]["pdf"]["path"]  = pdfp
        state["report"]["pdf"]["score"] = total_score
        state.setdefault("decision_log", []).append({
            "step":"pdf_node","result":"generated",
            "time": datetime.now().isoformat(),
            "details":{"path":pdfp,"score":total_score}
        })
    except Exception as e:
        state.setdefault("report", {}).setdefault("pdf", {})["generated"] = False
        state["report"]["pdf"]["error"] = str(e)
        state.setdefault("decision_log", []).append({
            "step":"pdf_node","result":"error",
            "time": datetime.now().isoformat(),
            "details":{"error":str(e)}
        })

    return state

# ì•¡ì…€ ë…¸ë“œ




# LangGraph ë¹Œë”
interview_builder = StateGraph(InterviewState)
interview_builder.add_node("stt_node", stt_node)
interview_builder.add_node("rewrite_agent", rewrite_agent)
interview_builder.add_node("rewrite_judge_agent", rewrite_judge_agent)
interview_builder.set_entry_point("stt_node")
interview_builder.add_edge("stt_node", "rewrite_agent")
interview_builder.add_edge("rewrite_agent", "rewrite_judge_agent")
interview_builder.add_conditional_edges(
    "rewrite_judge_agent", should_retry_rewrite,
    {"retry":"rewrite_agent", "done":"__end__"}
)
interview_flow_executor = interview_builder.compile()



final_builder = StateGraph(InterviewState)
final_builder.add_node("nonverbal_eval", nonverbal_evaluation_agent)
final_builder.add_node("evaluation_agent", evaluation_agent)
final_builder.add_node("evaluation_judge_agent", evaluation_judge_agent)
final_builder.add_node("pdf_node", pdf_node)
final_builder.set_entry_point("nonverbal_eval")
final_builder.add_edge("nonverbal_eval", "evaluation_agent")
final_builder.add_edge("evaluation_agent", "evaluation_judge_agent")
final_builder.add_conditional_edges(
    "evaluation_judge_agent", should_retry_evaluation,
    {"retry":"evaluation_agent", "continue":"pdf_node"}
)
# final_builder.add_channel("decision_log", LastValue())
final_report_flow_executor = final_builder.compile()