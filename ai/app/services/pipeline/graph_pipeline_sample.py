from langgraph.graph import StateGraph
from datetime import datetime
import os
import asyncio
from typing import TypedDict, Literal
import openai
from app.services.interview.stt_service import transcribe_audio_file
from app.services.interview.rewrite_service import rewrite_answer
from app.services.interview.evaluation_service import evaluate_keywords_from_full_answer
from app.services.interview.report_service import create_radar_chart, generate_pdf
from app.schemas.nonverbal import Posture, FacialExpression, NonverbalData
from app.services.interview.nonverbal_service import evaluate  
from app.schemas.state import InterviewState
from langgraph.channels import LastValue, BinaryOperatorAggregate



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Typed State â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# class InterviewState(TypedDict, total=False):
#     interviewee_id: str
#     audio_path: str
#     stt: dict
#     rewrite: dict
#     evaluation: dict
#     report: dict
#     decision_log: list

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Nodes / Agents â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# stt node
def stt_node(state: InterviewState) -> InterviewState:
    audio_path = state.get("audio_path")
    raw = transcribe_audio_file(audio_path)
    ts = datetime.now().isoformat()
    state.setdefault("stt", {"done": False, "segments": []})
    state["stt"]["segments"].append({"raw": raw, "timestamp": ts})
    state.setdefault("decision_log", []).append({
        "step": "stt_node",
        "result": "success",
        "time": ts,
        "details": {"segment": raw[:30]}
    })
    return state


# rewrite agent
async def rewrite_agent(state: InterviewState) -> InterviewState:
    raw = state["stt"]["segments"][-1]["raw"]
    rewritten, _ = await rewrite_answer(raw)
    item = {"raw": raw, "rewritten": rewritten}
    state.setdefault("rewrite", {"done": False, "items": []})
    state["rewrite"]["items"].append(item)
    ts = datetime.now().isoformat()
    state["decision_log"].append({
        "step": "rewrite_agent",
        "result": "processing",
        "time": ts,
        "details": {"raw_preview": raw[:30]}
    })
    return state

# ë¦¬ë¼ì´íŒ… ê²€ì¦ í›„ ì¡°ê±´ë¶€ ë¶„ê¸° í•¨ìˆ˜
def should_retry_rewrite(state: InterviewState) -> Literal["retry", "done"]:
    """
    ë¦¬ë¼ì´íŒ… ê²°ê³¼ ê²€ì¦ í›„ ì¬ì‹œë„ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ëŠ” í•¨ìˆ˜
    
    Returns:
        "retry": ì¬ë¦¬ë¼ì´íŒ… í•„ìš”
        "done": ì™„ë£Œ
    """
    items = state.get("rewrite", {}).get("items", [])
    
    # ëª¨ë“  í•­ëª©ì´ ok=Trueì¸ì§€ í™•ì¸
    all_passed = all(item.get("ok", False) for item in items)
    
    # ì¬ì‹œë„ íšŸìˆ˜ í™•ì¸
    retry_count = state.get("rewrite", {}).get("retry_count", 0)
    
    if not all_passed and retry_count < 3:  # ìµœëŒ€ 3ë²ˆê¹Œì§€ ì¬ì‹œë„
        state["rewrite"]["retry_count"] = retry_count + 1
        return "retry"
    
    return "done"

# rewrite judge agent
# rewrite judge agentì—ëŠ” ì•„ë˜ì™€ ê°™ì€ ê²€ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤.
# ì‹œìŠ¤í…œ: ë‹¹ì‹ ì€ í…ìŠ¤íŠ¸ ë¦¬ë¼ì´íŒ… í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
# ì›ë³¸: \"{raw}\"
# ë¦¬ë¼ì´íŒ…: \"{rewritten}\"
# 1) ì˜ë¯¸ ë³´ì¡´
# 2) ê³¼ì‰ ì¶•ì•½/í™•ì¥
# 3) ì˜¤íƒˆì/ë¬¸ë§¥ ì˜¤ë¥˜
# ìœ„ ê¸°ì¤€ì— ë”°ë¼ JSON í˜•ì‹ìœ¼ë¡œ ok(bool)ì™€ notes(list)ë¥¼ ë°˜í™˜í•˜ì„¸ìš”.
#  """
#     1) rewrite.items ê°€ ë¹„ì–´ ìˆìœ¼ë©´ ì—ëŸ¬
#     2) ê° itemì— ok, judge_notes ì„¤ì •
#     3) ok=Trueì¸ í•­ëª©ë§Œ rewrite.final ì— {raw, rewritten, timestamp} í˜•íƒœë¡œ ëˆ„ì 
#     4) rewrite.done = True, decision_log ê¸°ë¡
#     """
# # """

async def rewrite_judge_agent(state: InterviewState) -> InterviewState:
    """
    ë¦¬ë¼ì´íŒ…ëœ í…ìŠ¤íŠ¸ë¥¼ í‰ê°€í•˜ê³  ê²€ì¦í•˜ëŠ” ì—ì´ì „íŠ¸
    
    Args:
        state (InterviewState): í˜„ì¬ ë©´ì ‘ ìƒíƒœ
        
    Returns:
        InterviewState: ì—…ë°ì´íŠ¸ëœ ë©´ì ‘ ìƒíƒœ
    """
    # 1. rewrite.itemsê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
    if not state.get("rewrite", {}).get("items"):
        state.setdefault("decision_log", []).append({
            "step": "rewrite_judge_agent",
            "result": "error",
            "time": datetime.now().isoformat(),
            "details": {"error": "No rewrite items found"}
        })
        return state

    # 2. ê° item í‰ê°€
    for item in state["rewrite"]["items"]:
        if "ok" in item:  # ì´ë¯¸ í‰ê°€ëœ í•­ëª©ì€ ê±´ë„ˆë›°ê¸°
            continue
            
        raw = item["raw"]
        rewritten = item["rewritten"]

        prompt = JUDGE_PROMPT.format(raw=raw, rewritten=rewritten)

        try:
            start = time.perf_counter()
            response = openai.chat.completions.create(
                model="gpt-4",  # gpt-4o-miniëŠ” ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ëª…ì…ë‹ˆë‹¤
                messages=[{"role": "user", "content": prompt}],
                temperature=0,
                max_tokens=512
            )
            elapsed = time.perf_counter() - start
            result_json = response.choices[0].message.content.strip()

            # GPTê°€ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•œë‹¤ê³  ê°€ì •í•˜ê³  íŒŒì‹±
            import json
            result = json.loads(result_json)

            # ê²°ê³¼ë¥¼ itemì— ë°˜ì˜
            item["ok"] = result.get("ok", False)
            item["judge_notes"] = result.get("judge_notes", [])
            
            # 3. ok=Trueì¸ í•­ëª©ë§Œ finalì— ëˆ„ì 
            if item["ok"]:
                state.setdefault("rewrite", {}).setdefault("final", []).append({
                    "raw": raw,
                    "rewritten": rewritten,
                    "timestamp": datetime.now().isoformat()
                })

            # decision_log ê¸°ë¡
            state.setdefault("decision_log", []).append({
                "step": "rewrite_judge_agent",
                "result": f'ok={item["ok"]}',
                "time": datetime.now().isoformat(),
                "details": {
                    "notes": item["judge_notes"],
                    "elapsed_sec": round(elapsed, 2)
                }
            })

        except Exception as e:
            # ì‹¤íŒ¨ ì‹œ fallback
            item["ok"] = False
            item["judge_notes"] = [f"judge error: {str(e)}"]
            state.setdefault("decision_log", []).append({
                "step": "rewrite_judge_agent",
                "result": "error",
                "time": datetime.now().isoformat(),
                "details": {"error": str(e)}
            })

    # 4. ëª¨ë“  í•­ëª©ì´ ì²˜ë¦¬ë˜ì—ˆìœ¼ë©´ done í”Œë˜ê·¸ ì„¤ì •
    if all("ok" in item for item in state["rewrite"]["items"]):
        state["rewrite"]["done"] = True

    return state

# ë¹„ì–¸ì–´ì  ìš”ì†Œ í‰ê°€ agent
async def nonverbal_evaluation_agent(state: InterviewState) -> InterviewState:
    ts = datetime.now().isoformat()
    try:
        counts: Dict[str, Any] = state.get("nonverbal_counts", {})
        if not counts:
            raise ValueError("nonverbal_countsê°€ ì—†ìŠµë‹ˆë‹¤.")

        # 1) ì„¸ë¶€ ì¹´ìš´íŠ¸ íŒŒì‹±
        posture_data = counts["posture"]              # ì˜ˆ: {"leg_spread":2, "leg_shake":1, "head_down":0, ...}
        facial_data  = counts["expression"]           # ì˜ˆ: {"smile":3, "neutral":2, "frown":1, ...}

        posture = Posture.parse_obj(posture_data)
        facial  = FacialExpression.parse_obj(facial_data)

        # 2) NonverbalData ìƒì„± & AI í‰ê°€
        nv = NonverbalData(
            interviewee_id=state["interviewee_id"],
            is_speaking=False,  # end ì‹œì ì´ë¯€ë¡œ speaking ì—¬ë¶€ ë¬´ê´€
            posture=posture,
            facial_expression=facial
        )
        nv_score = await evaluate(nv)  # NonverbalScore

        # 3) 0.0â€“1.0 â†’ 0â€“15ì  í™˜ì‚°
        pts = int(round(nv_score.overall_score * 15))
        reason = nv_score.detailed_analysis or nv_score.feedback.get("ì¢…í•©", "")

        # 4) stateì— ë³‘í•©
        state.setdefault("evaluation", {}).setdefault("results", {})["ë¹„ì–¸ì–´ì "] = {
            "score": pts,
            "reason": reason
        }
        state.setdefault("decision_log", []).append({
            "step": "nonverbal_evaluation",
            "result": "success",
            "time": ts,
            "details": {"score": pts}
        })

    except Exception as e:
        state.setdefault("evaluation", {})["nonverbal_error"] = str(e)
        state.setdefault("decision_log", []).append({
            "step": "nonverbal_evaluation",
            "result": "error",
            "time": ts,
            "details": {"error": str(e)}
        })

    return state


# í‰ê°€ ê²€ì¦ í›„ ì¡°ê±´ë¶€ ë¶„ê¸° í•¨ìˆ˜
def should_retry_evaluation(state: InterviewState) -> Literal["retry", "continue"]:
    """
    í‰ê°€ ê²°ê³¼ ê²€ì¦ í›„ ì¬í‰ê°€ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ëŠ” í•¨ìˆ˜
    
    Returns:
        "retry": ì¬í‰ê°€ í•„ìš”
        "continue": ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰
    """
    evaluation = state.get("evaluation", {})
    
    # í‰ê°€ê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ê±°ë‚˜ ê²€ì¦ì— ì‹¤íŒ¨í•œ ê²½ìš°
    if not evaluation.get("done") or not evaluation.get("ok", False):
        # ì¬ì‹œë„ íšŸìˆ˜ í™•ì¸
        retry_count = evaluation.get("retry_count", 0)
        if retry_count < 3:  # ìµœëŒ€ 3ë²ˆê¹Œì§€ ì¬ì‹œë„
            # ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€
            state["evaluation"]["retry_count"] = retry_count + 1
            return "retry"
    
    return "continue"


# í‰ê°€ agent
async def evaluation_agent(state: InterviewState) -> InterviewState:
    """
    LLM í‚¤ì›Œë“œ ê¸°ë°˜ í‰ê°€ + ë¹„ì–¸ì–´ì  ìš”ì†Œ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    ë¹„ì–¸ì–´ì  ìš”ì†Œ í‰ê°€ ê´€ë ¨ ì¶”ê°€ í•„ìš®
    """
    rewritten_items = state.get("rewrite", {}).get("items", [])
    full_answer = "\n".join([item["rewritten"] for item in rewritten_items])
    results = await evaluate_keywords_from_full_answer(full_answer)
    state["evaluation"] = {
        "done": True,
        "results": results
    }
    ts = datetime.now().isoformat()
    state.setdefault("decision_log", []).append({
        "step": "evaluation_agent",
        "result": "done",
        "time": ts,
        "details": {}
    })
    return state



# í‰ê°€ judge agent
async def evaluation_judge_agent(state: InterviewState) -> InterviewState:
    """
    í‰ê°€ ê²°ê³¼ë¥¼ ê²€ì¦í•˜ëŠ” ì—ì´ì „íŠ¸

    ê²€ì¦ ê¸°ì¤€:
    1. í‚¤ì›Œë“œë³„ í•­ëª© ìˆ˜ê°€ ì˜¬ë°”ë¥¸ì§€ (3ê°œ)
    2. ì ìˆ˜ ë²”ìœ„ê°€ 1~5ì ì¸ì§€
    3. ì´ì ì´ ì˜¬ë°”ë¥¸ ë²”ìœ„ ë‚´ì¸ì§€ (ë™ì  max ê¸°ì¤€)

    Args:
        state (InterviewState): í˜„ì¬ ë©´ì ‘ ìƒíƒœ

    Returns:
        InterviewState: ì—…ë°ì´íŠ¸ëœ ë©´ì ‘ ìƒíƒœ
    """
    if not state.get("evaluation", {}).get("results"):
        state.setdefault("decision_log", []).append({
            "step": "evaluation_judge_agent",
            "result": "error",
            "time": datetime.now().isoformat(),
            "details": {"error": "No evaluation results found"}
        })
        return state

    results = state["evaluation"]["results"]
    judge_notes = []
    is_valid = True

    # 1. í‚¤ì›Œë“œë³„ í•­ëª© ìˆ˜ ê²€ì¦ (3ê°œ)
    for keyword, criteria in results.items():
        if len(criteria) != 3:
            judge_notes.append(f"Keyword '{keyword}' has {len(criteria)} criteria (expected 3)")
            is_valid = False

    # 2. ì ìˆ˜ ë²”ìœ„ ê²€ì¦ (1~5ì )
    for keyword, criteria in results.items():
        for criterion, data in criteria.items():
            score = data.get("score", 0)
            if not (1 <= score <= 5):
                judge_notes.append(f"Invalid score {score} in {keyword}.{criterion}")
                is_valid = False

    # 3. ì´ì  ë²”ìœ„ ê²€ì¦
    total_score = sum(
        sum(criterion.get("score", 0) for criterion in criteria.values())
        for criteria in results.values()
    )
    max_possible_score = len(results) * 3 * 5  # í‚¤ì›Œë“œ ìˆ˜ * 3ê°œ í•­ëª© * ìµœëŒ€ ì ìˆ˜ 5

    if total_score > max_possible_score:
        judge_notes.append(f"Total score {total_score} exceeds maximum {max_possible_score}")
        is_valid = False

    # í‰ê°€ ê²°ê³¼ ìƒíƒœì— ê¸°ë¡
    state["evaluation"]["judge"] = {
        "ok": is_valid,
        "judge_notes": judge_notes,
        "total_score": total_score,
        "max_score": max_possible_score
    }

    # ìƒìœ„ í‰ê°€ ìƒíƒœì—ë„ ë°˜ì˜
    state["evaluation"]["ok"] = is_valid

    # decision_log ê¸°ë¡
    state.setdefault("decision_log", []).append({
        "step": "evaluation_judge_agent",
        "result": f"ok={is_valid}",
        "time": datetime.now().isoformat(),
        "details": {
            "total_score": total_score,
            "max_score": max_possible_score,
            "keywords_checked": len(results),
            "notes": judge_notes
        }
    })

    return state



async def pdf_node(state: InterviewState) -> InterviewState:
    from app.constants.evaluation_constants_full_all import (
        TECHNICAL_EVAL_CRITERIA_WITH_ALL_SCORES,
        DOMAIN_EVAL_CRITERIA_WITH_ALL_SCORES,
    )

    answers = [item["rewritten"] for item in state.get("rewrite", {}).get("items", [])]
    evaluation_results = state.get("evaluation", {}).get("results", {})
    questions = state.get("questions", [])

    if not evaluation_results:
        state.setdefault("report", {}).setdefault("pdf", {})["generated"] = False
        state["report"]["pdf"]["error"] = "í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        return state

    keyword_results = {
        kw: {
            "score": sum(sub.get("score", 0) for sub in criteria.values()),
            "reasons": "\n".join(sub.get("reason", "") for sub in criteria.values())
        }
        for kw, criteria in evaluation_results.items()
    }

    cid = state.get("interviewee_id")
    ts = datetime.now().strftime('%Y%m%d%H%M%S')
    out_dir = './results'
    os.makedirs(out_dir, exist_ok=True)
    chart_path = os.path.join(out_dir, f"{cid}_chart_{ts}.png")
    pdf_path = os.path.join(out_dir, f"{cid}_report_{ts}.pdf")

    try:
        technical_keywords = set(TECHNICAL_EVAL_CRITERIA_WITH_ALL_SCORES.keys())
        domain_keywords = set(DOMAIN_EVAL_CRITERIA_WITH_ALL_SCORES.keys())

        technical_total = 0
        domain_total = 0
        general_total = 0

        nonverbal_score = 0
        nonverbal_reason = ""
        if "ë¹„ì–¸ì–´ì " in keyword_results:
            nonverbal_score = keyword_results.pop("ë¹„ì–¸ì–´ì ")["score"]
            nonverbal_reason = keyword_results.pop("ë¹„ì–¸ì–´ì ", {}).get("reasons", "")

        for k, v in keyword_results.items():
            if k in technical_keywords:
                technical_total += v["score"]
            elif k in domain_keywords:
                domain_total += v["score"]
            else:
                general_total += v["score"]

        final_score = round(
            (general_total / 105 * 67.5) +
            (technical_total / 15 * 22.5) +
            (domain_total / 15 * 22.5) +
            (nonverbal_score / 15 * 15)
        )

        create_radar_chart(keyword_results, chart_path)
        generate_pdf(
            keyword_results=keyword_results,
            chart_path=chart_path,
            output_path=pdf_path,
            interviewee_id=cid,
            questions=questions,
            answers=answers,
            nonverbal_score=nonverbal_score,
            nonverbal_reason=nonverbal_reason,
            total_score=final_score
        )

        state.setdefault("report", {}).setdefault("pdf", {})["generated"] = True
        state["report"]["pdf"]["path"] = pdf_path
        ts2 = datetime.now().isoformat()
        state["decision_log"].append({
            "step": "pdf_node",
            "result": "generated",
            "time": ts2,
            "details": {"path": pdf_path}
        })
    except Exception as e:
        state.setdefault("report", {}).setdefault("pdf", {})["generated"] = False
        state["report"]["pdf"]["error"] = str(e)
        ts2 = datetime.now().isoformat()
        state["decision_log"].append({
            "step": "pdf_node",
            "result": "error",
            "time": ts2,
            "details": {"error": str(e)}
        })
    return state


# excel node 
# async def excel_node(state: InterviewState) -> InterviewState:
#     # ì§€ì›ìë³„ ì§€ì›ì ì•„ì´ë””, ì§€ì›ìë³„ 5ê°œ ì§ˆë¬¸ ë°ì´í„°, ì§€ì›ì ë‹µë³€ raw stt data, ì§€ì›ì ë‹µë³€ rewritten data, ì§€ì›ì ì´ì  
#     # ìœ„ì˜ í•­ëª©ë“¤ì„ ì¹¼ëŸ¼ìœ¼ë¡œ í•˜ëŠ” excel íŒŒì¼ ìƒì„±
#     # íŒŒì¼ ì´ë¦„ì€ ì§€ì›ì ì•„ì´ë””_report_YYYYMMDDHHMMSS.xlsx í˜•ì‹ìœ¼ë¡œ ìƒì„±
#     # íŒŒì¼ ê²½ë¡œëŠ” ./results í´ë”ì— ìƒì„±
    
#     state.setdefault("report", {})["excel"] = {
#         "generated": False,
#         "path": None
#     }
#     return state



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LangGraph ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

interview_builder = StateGraph(InterviewState)
interview_builder.add_node("stt_node", stt_node)
interview_builder.add_node("rewrite_agent", rewrite_agent)  # asyncì—¬ë„ ê·¸ëƒ¥ ë“±ë¡
interview_builder.add_node("rewrite_judge_agent", rewrite_judge_agent)  # asyncì—¬ë„ ê·¸ëƒ¥ ë“±ë¡
interview_builder.set_entry_point("stt_node")
interview_builder.add_edge("stt_node", "rewrite_agent")
interview_builder.add_edge("rewrite_agent", "rewrite_judge_agent")
interview_builder.add_conditional_edges(
    "rewrite_judge_agent",
    should_retry_rewrite,
    {
        "retry": "rewrite_agent",
        "done": "__end__"
    }
)
interview_flow_executor = interview_builder.compile()

# ğŸ“„ ë©´ì ‘ ì¢…ë£Œ: ì „ì²´ í‰ê°€ ë° ë¦¬í¬íŠ¸ ìƒì„±
final_builder = StateGraph(InterviewState)
final_builder.add_node("nonverbal_eval", nonverbal_evaluation_agent)
final_builder.add_node("evaluation_agent", evaluation_agent)
final_builder.add_node("evaluation_judge_agent", evaluation_judge_agent)
final_builder.add_node("pdf_node", pdf_node)
final_builder.set_entry_point("nonverbal_eval")
final_builder.add_edge("nonverbal_eval", "evaluation_agent")
final_builder.add_edge("evaluation_agent", "evaluation_judge_agent")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ decision_log ì±„ë„ ì¬ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€
# (A) ìµœê·¼ 1ê±´ë§Œ ë‚¨ê¸°ê¸°
final_builder.set_channel(
    "decision_log",
    LastValue()
)

# ë˜ëŠ”

# (B) ë§ˆì§€ë§‰ 20ê±´ë§Œ ìœ ì§€í•˜ê¸°
def reducer(old: list, new: dict) -> list:
    return (old + [new])[-20:]

final_builder.set_channel(
    "decision_log",
    BinaryOperatorAggregate(list, reducer)
)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# conditional ë¶„ê¸°ë§Œ ë‚¨ê¸°ê³  direct edge ì œê±°
final_builder.add_conditional_edges(
    "evaluation_judge_agent",
    should_retry_evaluation,
    {"retry": "evaluation_agent", "continue": "pdf_node"}
)

final_report_flow_executor = final_builder.compile()