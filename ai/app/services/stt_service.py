from pydub import AudioSegment
from openai import OpenAI
import os
from dotenv import load_dotenv
import uuid
from typing import List, Dict
import json
from datetime import datetime
import time

# ğŸ“¦ .env í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
dotenv_path = os.path.join(os.path.dirname(__file__), '..', '..', '.env')
load_dotenv(dotenv_path)

# ğŸ”‘ OpenAI API Key ë¡œë“œ ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai_key = os.getenv("OPENAI_API_KEY")
if not openai_key:
    raise ValueError("âŒ OPENAI_API_KEYê°€ .envì— ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
client = OpenAI(api_key=openai_key)

# ğŸ”„ ì˜¤ë””ì˜¤ í¬ë§·ì„ Whisper API í˜¸í™˜ wavë¡œ ë³€í™˜
def convert_audio_to_wav(input_path: str, output_path: str):
    """
    ì…ë ¥ ì˜¤ë””ì˜¤ íŒŒì¼ì„ 16kHz, mono wav í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    """
    audio = AudioSegment.from_file(input_path)
    audio = audio.set_frame_rate(16000).set_channels(1)
    audio.export(output_path, format="wav")

# ğŸ§  OpenAI Whisper APIë¥¼ í†µí•œ STT ìˆ˜í–‰
def transcribe_audio_file(file_path: str) -> str:
    """
    Whisper APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ì „ì‚¬í•¨
    """
    with open(file_path, "rb") as f:
        transcript = client.audio.transcriptions.create(
            model="whisper-1",
            file=f,
            response_format="text"  # ë˜ëŠ” "json"
        )
    return transcript.strip()

UPLOAD_DIR = os.path.join(os.path.dirname(__file__), "../routers/uploads")
os.makedirs(UPLOAD_DIR, exist_ok=True)

# STT ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
transcription_chunks = []

def process_audio_chunk(audio_chunk: AudioSegment) -> str:
    """10ì´ˆ ë‹¨ìœ„ì˜ ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ ì²˜ë¦¬"""
    unique_id = str(uuid.uuid4())
    temp_path = os.path.join(UPLOAD_DIR, f"{unique_id}.wav")
    
    audio_chunk.export(temp_path, format="wav")
    text = transcribe_audio_file(temp_path)
    
    # ì„ì‹œ íŒŒì¼ ì •ë¦¬
    os.remove(temp_path)
    
    return text

def add_transcription(text: str):
    """ìƒˆë¡œìš´ STT ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€"""
    transcription_chunks.append(text)

def get_all_transcriptions() -> List[str]:
    """ëª¨ë“  STT ê²°ê³¼ ë°˜í™˜"""
    return transcription_chunks

def clear_transcriptions():
    """STT ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”"""
    transcription_chunks.clear()

# from pydub import AudioSegment
# import whisper
# from pyannote.audio import Pipeline
# import os
# from dotenv import load_dotenv
#
# # Whisper ëª¨ë¸ ë¡œë“œ
# model = whisper.load_model("small")
#
# # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# dotenv_path = os.path.join(os.path.dirname(__file__), '..', '..', '.env')
# #load_dotenv()
# load_dotenv(dotenv_path)
# HUGGINGFACE_TOKEN = os.getenv("HF_TOKEN")
# print(f"HUGGINGFACE_TOKEN: {HUGGINGFACE_TOKEN[:8]}..." if HUGGINGFACE_TOKEN else "âŒ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ì‹¤íŒ¨")
#
# # pyannote í™”ì ë¶„ë¦¬ íŒŒì´í”„ë¼ì¸
# diarization_pipeline = Pipeline.from_pretrained(
#     "pyannote/speaker-diarization",
#     use_auth_token=HUGGINGFACE_TOKEN
# )
#
# def convert_webm_to_wav(webm_path, wav_path):
#     audio = AudioSegment.from_file(webm_path, format="webm")
#     audio.export(wav_path, format="wav")
# # ì˜¤ë””ì˜¤ ë³€í™˜ (í™•ì¥ì ë¬´ê´€)
# def convert_audio_to_wav(input_path: str, output_path: str):
#     audio = AudioSegment.from_file(input_path)
#     audio = audio.set_frame_rate(16000).set_channels(1)  # ìƒ˜í”Œë ˆì´íŠ¸ ë° ì±„ë„ ì„¤ì •
#     audio.export(output_path, format="wav")
#
# # Whisper ë‹¨ìˆœ ì „ì‚¬
# def transcribe_audio_file(file_path: str) -> str:
#     result = model.transcribe(file_path)
#     return result["text"]
#
# # Whisper + pyannote í™”ì ë¶„ë¦¬ ì „ì‚¬
# def transcribe_audio_file_with_speaker_labels(wav_path: str) -> list:
#     diarization = diarization_pipeline(wav_path)
#     result = model.transcribe(wav_path, verbose=False)
#
#     segments = []
#     for turn in diarization.itertracks(yield_label=True):
#         start, end = turn[0].start, turn[0].end
#         speaker = turn[2]
#         spoken_texts = [
#             seg['text'] for seg in result['segments']
#             if not (seg['end'] < start or seg['start'] > end)
#         ]
#         combined = ' '.join(spoken_texts).strip()
#         if combined:
#             segments.append({
#                 "speaker": speaker,
#                 "start": round(start, 1),
#                 "end": round(end, 1),
#                 "text": combined
#             })
#     return segments

# '''
# # ------------------- í™”ì ë¶„ë¦¬ ì½”ë“œ (ì£¼ì„) -------------------
# # from openai import OpenAI  # âœ… ìƒˆë¡œìš´ ë°©ì‹
# # import json
# # from dotenv import load_dotenv
# #
# # load_dotenv()  # .env ë¡œë“œ
# #
# # # âœ… ìƒˆë¡œìš´ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
# # client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
# #
# # # ğŸ§  GPTì—ê²Œ ì—­í•  ë¶„ë¥˜ ìš”ì²­
# # def classify_speakers_with_gpt(speaker_segments: list) -> dict:
# #     dialogue = "\n".join(f"{seg['speaker']}: {seg['text']}" for seg in speaker_segments)
# #
# #     prompt = (
# #         "ë‹¤ìŒì€ í™”ì ë¶„ë¦¬ëœ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤:\n\n"
# #         f"{dialogue}\n\n"
# #         "ê° SPEAKERê°€ 'ë©´ì ‘ê´€'ì¸ì§€ 'ì§€ì›ì'ì¸ì§€ ë¶„ë¥˜í•´ì£¼ì„¸ìš”.\n"
# #         "ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ JSONìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:\n"
#         "{ \"SPEAKER_00\": \"ë©´ì ‘ê´€\", \"SPEAKER_01\": \"ì§€ì›ì\" }"
#     )
#
#     try:
#         response = client.chat.completions.create(
#         model="gpt-4o-mini",  # ë˜ëŠ” "gpt-3.5-turbo"
#         messages=[{"role": "user", "content": prompt}],
#         temperature=0,
#         )
#         return json.loads(response.choices[0].message.content)
#     except Exception as e:
#         print("[GPT ì˜¤ë¥˜]", e)
#         return {}
#
# # ğŸ” ê°™ì€ í™”ì ë¸”ë¡ ë¬¶ê¸°
# def group_by_speaker(segments: list, speaker_map: dict) -> str:
#     result_lines = []
#     current_speaker = None
#     current_text = ""
#
#     for seg in segments:
#         spk = seg["speaker"]
#         role = speaker_map.get(spk, spk)
#         text = seg["text"].strip()
#
#         if spk != current_speaker:
#             if current_speaker is not None:
#                 result_lines.append(f"{speaker_map[current_speaker]}: {current_text.strip()}")
#             current_speaker = spk
#             current_text = text
#         else:
#             current_text += " " + text
#
#     if current_speaker and current_text:
#         result_lines.append(f"{speaker_map[current_speaker]}: {current_text.strip()}")
#
#     return "\n".join(result_lines)
#
# @router.post("/")
# async def stt_with_diarization(audio: UploadFile = File(...)):
#     input_path = os.path.join(UPLOAD_DIR, audio.filename)
#
#     with open(input_path, "wb") as buffer:
#         shutil.copyfileobj(audio.file, buffer)
#
#     try:
#         base_name = os.path.splitext(input_path)[0]
#         wav_path = base_name + ".wav"
#
#         convert_audio_to_wav(input_path, wav_path)
#         speaker_segments = transcribe_audio_file_with_speaker_labels(wav_path)
#
#         # ğŸ§  GPTë¡œ í™”ì ì—­í•  íŒë‹¨
#         speaker_map = classify_speakers_with_gpt(speaker_segments)
#
#         # ğŸ“œ ë¬¸ì¥ ë¬¶ê¸°
#         result_text = group_by_speaker(speaker_segments, speaker_map)
#
#         os.remove(input_path)
#         os.remove(wav_path)
#
#         return {"result": result_text}
#
#     except Exception as e:
#         return {"error": str(e)}
# # ----------------------------------------------------------
# load_dotenv()
# openai_key = os.getenv("OPENAI_API_KEY")

# model = whisper.load_model("base")
# openai = OpenAI(api_key=openai_key)

# def convert_webm_to_wav(webm_path, wav_path):
#     audio = AudioSegment.from_file(webm_path, format="webm")
#     audio.export(wav_path, format="wav")

# def transcribe_audio_file(wav_path: str) -> str:
#     try:
#         result = model.transcribe(wav_path)
#         raw_text = result["text"]

#         prompt = f"""
#         ë‹¤ìŒì€ Whisperë¡œ ì¶”ì¶œí•œ ë©´ì ‘ ë‹µë³€ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤:

#         "{raw_text}"

#         ì•„ë˜ ê¸°ì¤€ì— ë§ê²Œ ë©´ì ‘ ë‹µë³€ ìŠ¤íƒ€ì¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ë“¬ì–´ ì£¼ì„¸ìš”:
#         - ë©´ì ‘ê´€ì˜ ì§ˆë¬¸ì€ ì œì™¸í•˜ê³ , ë©´ì ‘ìì˜ ë‹µë³€ë§Œ í¬í•¨
#         - ë¬¸ë§¥ ìœ ì§€
#         - ì˜¤íƒ€, ë„ì–´ì“°ê¸° ìˆ˜ì •
#         - ì˜ë¯¸ ì—†ëŠ” íŠ¹ìˆ˜ë¬¸ì ì œê±°
#         - ì–´ìƒ‰í•œ í‘œí˜„ì€ ê³µì‹ì ì´ê³  ë¶€ë“œëŸ½ê²Œ ê³ ì¹˜ê¸°
#         - ì‰¼í‘œì™€ ë§ˆì¹¨í‘œ ë“± ë¬¸ì¥ ë¶€í˜¸ ë³´ì™„

#         ë‹¤ë“¬ì–´ì§„ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•´ ì£¼ì„¸ìš”.
#         """

#         response = openai.chat.completions.create(
#             model="gpt-4o",
#             messages=[{"role": "user", "content": prompt}],
#             temperature=0.0,
#         )

#         return response.choices[0].message.content.strip()

#     except Exception as e:
#         raise Exception(f"ìŒì„± ì¸ì‹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")