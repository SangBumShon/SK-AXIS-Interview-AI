from pydub import AudioSegment
from openai import OpenAI
import os
from dotenv import load_dotenv
from fastapi import UploadFile
import whisper
from typing import Optional
from datetime import datetime
import numpy as np


# ğŸ“¦ .env í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
dotenv_path = os.path.join(os.path.dirname(__file__), '..', '..', '.env')
load_dotenv(dotenv_path)

# ğŸ”‘ OpenAI API Key ë¡œë“œ ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai_key = os.getenv("OPENAI_API_KEY")
if not openai_key:
    raise ValueError("âŒ OPENAI_API_KEYê°€ .envì— ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
# OpenAI Python v1.x í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=openai_key)

# Whisper ëª¨ë¸ ì´ˆê¸°í™”
model = whisper.load_model("base")

#ğŸ§  OpenAI Whisper APIë¥¼ í†µí•œ STT ìˆ˜í–‰
def transcribe_audio_file(file_path: str) -> str:
    """
    Whisper APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ì „ì‚¬í•¨
    """
    # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì ìš©
    processed_path = preprocess_audio(file_path)
    
    with open(processed_path, "rb") as f:
        transcript = client.audio.transcriptions.create(
            model="whisper-1",
            file=f,
            response_format="text",
            language="ko",
            # í”„ë¡¬í”„íŠ¸ ì¶”ê°€ë¡œ ë§¥ë½ ì œê³µ
        )
    
    # ì „ì²˜ë¦¬ëœ ì„ì‹œ íŒŒì¼ ì‚­ì œ
    if processed_path != file_path and os.path.exists(processed_path):
        os.remove(processed_path)
    
    # response_format="text" ë¥¼ ì‚¬ìš©í•˜ë©´ ë¬¸ìì—´ì´ ë°˜í™˜ë©ë‹ˆë‹¤.
    result = transcript.strip()
    
    # í›„ì²˜ë¦¬: ëª…ë°±íˆ ì˜ëª»ëœ ë³€í™˜ í•„í„°ë§
    if is_invalid_transcription(result):
        print(f"[STT í›„ì²˜ë¦¬] ì˜ëª»ëœ ë³€í™˜ ê°ì§€: {result}")
        return "ìŒì„±ì„ ëª…í™•í•˜ê²Œ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    return result

def is_invalid_transcription(text: str) -> bool:
    """
    ëª…ë°±íˆ ì˜ëª»ëœ STT ê²°ê³¼ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
    """
    if not text or len(text.strip()) == 0:
        return True
    
    # ì˜ëª»ëœ ë³€í™˜ íŒ¨í„´ë“¤
    invalid_patterns = [
        "ì‹œì²­í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤",
        "ì‹œì²­ í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤",
        "ì˜¤ëŠ˜ë„ ì˜ìƒ ì‹œì²­ í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤",
        "ë¨¹ë°©",
        "ë¹ ì´ë¹ ì´", 
        "êµ¬ë…",
        "ì˜ìƒ ì‹œì²­",
        "ì±„ë„",
        "ìœ íŠœë¸Œ"
    ]
    
    text_lower = text.lower()
    for pattern in invalid_patterns:
        if pattern in text_lower:
            return True
    
    # ë„ˆë¬´ ì§§ì€ ì˜ë¯¸ì—†ëŠ” ë‹¨ì–´ë“¤
    if len(text.strip()) < 3:
        return True
        
    return False

async def save_audio_file(interviewee_id: int, audio_file: UploadFile) -> Optional[str]:
    """
    ì—…ë¡œë“œëœ ì˜¤ë””ì˜¤ íŒŒì¼ì„ uploads ë””ë ‰í† ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        interviewee_id: ë©´ì ‘ì ID
        audio_file: ì—…ë¡œë“œëœ WebM ì˜¤ë””ì˜¤ íŒŒì¼
        
    Returns:
        str: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    try:
        # uploads ë””ë ‰í† ë¦¬ ìƒì„±
        save_dir = os.path.join("uploads")
        os.makedirs(save_dir, exist_ok=True)
        
        # íŒŒì¼ëª… ìƒì„± (interviewee_id_timestamp.webm í˜•ì‹)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{interviewee_id}_{timestamp}.webm"
        
        # íŒŒì¼ ì €ì¥
        file_path = os.path.join(save_dir, filename)
        with open(file_path, "wb") as buffer:
            content = await audio_file.read()
            buffer.write(content)
        
        return file_path
        
    except Exception as e:
        print(f"íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def preprocess_audio(file_path: str) -> str:
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì „ì²˜ë¦¬í•˜ì—¬ STT ì •í™•ë„ë¥¼ ê°œì„ í•©ë‹ˆë‹¤.
    """
    try:
        # pydubìœ¼ë¡œ ì˜¤ë””ì˜¤ ë¡œë“œ
        audio = AudioSegment.from_file(file_path)
        
        # 1. ìƒ˜í”Œë§ ë ˆì´íŠ¸ ì •ê·œí™” (16kHzê°€ Whisperì— ìµœì )
        if audio.frame_rate != 16000:
            audio = audio.set_frame_rate(16000)
            print(f"[ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬] ìƒ˜í”Œë§ ë ˆì´íŠ¸ ë³€ê²½: {audio.frame_rate} -> 16000Hz")
        
        # 2. ëª¨ë…¸ ì±„ë„ë¡œ ë³€í™˜
        if audio.channels > 1:
            audio = audio.set_channels(1)
            print(f"[ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬] ìŠ¤í…Œë ˆì˜¤ -> ëª¨ë…¸ ë³€í™˜")
        
        # 3. ìŒëŸ‰ ì •ê·œí™” (ë„ˆë¬´ ì‘ê±°ë‚˜ í° ì†Œë¦¬ ì¡°ì ˆ)
        if audio.dBFS < -30:  # ë„ˆë¬´ ì‘ì€ ì†Œë¦¬
            audio = audio + (abs(audio.dBFS) - 20)
            print(f"[ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬] ìŒëŸ‰ ì¦í­: {audio.dBFS}dB")
        elif audio.dBFS > -10:  # ë„ˆë¬´ í° ì†Œë¦¬
            audio = audio - (audio.dBFS + 10)
            print(f"[ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬] ìŒëŸ‰ ê°ì†Œ: {audio.dBFS}dB")
        
        # 4. ë¬´ìŒ êµ¬ê°„ ì œê±° (ì•ë’¤ 0.5ì´ˆ ì´ìƒ ë¬´ìŒ ì œê±°)
        audio = audio.strip_silence(silence_len=500, silence_thresh=-40)
        
        # 5. ì „ì²˜ë¦¬ëœ íŒŒì¼ ì €ì¥
        processed_path = file_path.replace(".webm", "_processed.wav")
        audio.export(processed_path, format="wav")
        
        print(f"[ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬] ì™„ë£Œ: {processed_path}")
        return processed_path
        
    except Exception as e:
        print(f"[ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬] ì˜¤ë¥˜ ë°œìƒ: {e}")
        return file_path  # ì „ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ì›ë³¸ íŒŒì¼ ë°˜í™˜
