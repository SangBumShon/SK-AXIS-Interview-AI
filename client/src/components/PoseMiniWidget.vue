<template>
  <div style="position:relative; width:100%; height:100%;">
    <video
      ref="video"
      width="1280" height="720"
      autoplay
      muted
      playsinline
      style="position:absolute; top:0; left:0; width:100%; height:100%; object-fit:contain; z-index:1; background:#111;"
    ></video>
    <canvas
      ref="canvas"
      width="1280" height="720"
      style="position:absolute; top:0; left:0; width:100%; height:100%; z-index:100; pointer-events:none; background:transparent; border: 2px solid red;"
    ></canvas>
  </div>
</template>

<script setup>
import { ref, onMounted, onBeforeUnmount, defineProps, watch, defineEmits, defineExpose } from 'vue'
import * as faceapi from 'face-api.js'

const props = defineProps({
  intervieweeNames: {
    type: Array,
    required: true,
    default: () => []
  },
  intervieweeIds: {
    type: Array,
    required: true,
    default: () => []
  }
})

const emit = defineEmits(['updateNonverbalData'])

// ë¶€ëª¨ ì»´í¬ë„ŒíŠ¸ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡ ëˆ„ì  ë°ì´í„° ë…¸ì¶œ
defineExpose({
  getAccumulatedNonverbalData: () => accumulatedNonverbalData.value,
  getCurrentNonverbalData: () => nonverbalData.value,
  stopDetection: () => {
    active = false
    console.log('[ê°ì§€ ì¤‘ë‹¨] PoseMiniWidget ê°ì§€ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.')
  }
})

// ë…¹ìŒ ê´€ë ¨ ìƒíƒœ - ë©´ì ‘ìë³„ ê°œë³„ ê´€ë¦¬
const recorderMap = ref({})  // { [id]: { mediaRecorder, audioChunks, stream } }
const MOUTH_CLOSED_THRESHOLD = 5000 // 5ì´ˆ

const video = ref(null)
const canvas = ref(null)
let active = true

// ì–¼êµ´í‘œì • ê´€ë ¨ ìƒìˆ˜
const expList = ['ë¯¸ì†Œ', 'ë¬´í‘œì •', 'ìš¸ìƒ', 'ì°¡ê·¸ë¦¼']
const expKorean = {
  happy: 'ë¯¸ì†Œ', sad: 'ìš¸ìƒ', angry: 'ì°¡ê·¸ë¦¼',
  neutral: 'ë¬´í‘œì •', disgusted: 'ë¶ˆì¾Œ'
}

// ê° ë©´ì ‘ìë³„ ìƒíƒœ ê´€ë¦¬
const faceStates = ref([])

// ë¹„ì–¸ì–´ì  ë°ì´í„° ì €ì¥ì†Œ
const nonverbalData = ref({})

// ë©´ì ‘ ì¢…ë£Œ ì‹œ ëˆ„ì  ë°ì´í„° ì €ì¥ì†Œ
const accumulatedNonverbalData = ref({})  // { [id]: { facial_expression_history: [], posture_history: [], ... } }

// 1ì´ˆë§ˆë‹¤ ë°ì´í„° ì—…ë°ì´íŠ¸ ë° ì „ì†¡
let updateInterval = null

// ë©´ì ‘ì ì´ë¦„ì´ ë³€ê²½ë  ë•Œë§ˆë‹¤ ìƒíƒœ ì´ˆê¸°í™”
watch(() => props.intervieweeNames, (newNames) => {
  faceStates.value = newNames.map((name, index) => {
    const id = props.intervieweeIds[index]
    nonverbalData.value[id] = {
      posture: { upright: 0, leaning: 0, slouching: 0 },
      facial_expression: { smile: 0, neutral: 0, frown: 0, angry: 0 },
      gaze: 0,
      gesture: 0,
      timestamp: Date.now()
    }
    
    // ëˆ„ì  ë°ì´í„° ì´ˆê¸°í™”
    accumulatedNonverbalData.value[id] = {
      facial_expression_history: [],
      posture_history: [],
      gaze_history: [],
      gesture_history: [],
      start_time: Date.now()
    }
    
    return {
      name,
      id,
      speaking: false,
      mouthClosedStartTime: null,
      isRecording: false,
      expression: Object.fromEntries(expList.map(e => [e, 0])),
      expressionTotal: 0, // ì´ í”„ë ˆì„ ìˆ˜
      lastExpression: null
    }
  })
}, { immediate: true })

function detectSpeaking(landmarks) {
  if (!landmarks || !landmarks.positions) return false
  const topLip = landmarks.positions[62]
  const bottomLip = landmarks.positions[66]
  const leftMouth = landmarks.positions[60]
  const rightMouth = landmarks.positions[64]
  if (!topLip || !bottomLip || !leftMouth || !rightMouth) return false
  const mouthOpen = Math.abs(topLip.y - bottomLip.y)
  const mouthWidth = Math.abs(leftMouth.x - rightMouth.x)
  return mouthOpen > 10 && mouthWidth > 20
}

async function startRecording(personIndex) {
  const state = faceStates.value[personIndex]
  
  if (state.isRecording) {
    console.log(`[ë…¹ìŒ ì‹œì‘ ì‹¤íŒ¨] ${state.name}ë‹˜ì˜ ë…¹ìŒì´ ì´ë¯¸ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.`)
    return
  }
  
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
    
    // WebM í˜•ì‹ìœ¼ë¡œ ë…¹ìŒ ì„¤ì •
    const mimeType = 'audio/webm'
    if (!MediaRecorder.isTypeSupported(mimeType)) {
      console.error(`[ë…¹ìŒ ì‹œì‘ ì‹¤íŒ¨] ${mimeType} í˜•ì‹ì´ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.`)
      return
    }
    
    const recorder = new MediaRecorder(stream, {
      mimeType: mimeType
    })
    const audioChunks = []
    
    recorder.ondataavailable = (event) => {
      audioChunks.push(event.data)
    }
    
    recorder.onstop = async () => {
      const audioBlob = new Blob(audioChunks, { type: mimeType })
      const timestamp = new Date().toISOString().replace(/[:.]/g, '-')
      const fileName = `${state.id}_${timestamp}.webm`
      
      const formData = new FormData()
      formData.append('audio', audioBlob, fileName)
      formData.append('interviewee_id', state.id.toString())
      
      try {
        const response = await fetch('http//:localhost:8000/api/v1/stt/upload', {
          method: 'POST',
          body: formData
        })
        
        if (!response.ok) {
          const errorData = await response.json().catch(() => ({}))
          throw new Error(`Upload failed: ${response.status} ${errorData.detail || response.statusText}`)
        }
        
        const result = await response.json()
        console.log(`[ì—…ë¡œë“œ ì„±ê³µ] ${state.name}ë‹˜ì˜ ë…¹ìŒ íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.`)
        state.isRecording = false
      } catch (error) {
        console.error(`[ì—…ë¡œë“œ ì‹¤íŒ¨] ${state.name}ë‹˜ì˜ ë…¹ìŒ íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:`, error.message)
        state.isRecording = false
      } finally {
        if (recorder && recorder.stream) {
          recorder.stream.getTracks().forEach(track => {
            track.stop()
          })
        }
      }
    }
    
    recorder.start()
    state.isRecording = true
    console.log(`[ë…¹ìŒ ì‹œì‘] ${state.name}ë‹˜ì˜ ë…¹ìŒì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.`)

    recorderMap.value[state.id] = { mediaRecorder: recorder, audioChunks, stream }
  } catch (error) {
    console.error(`[ë…¹ìŒ ì‹œì‘ ì‹¤íŒ¨] ${state.name}ë‹˜ì˜ ë…¹ìŒ ì‹œì‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:`, error.message)
  }
}

function stopRecording(personIndex) {
  const state = faceStates.value[personIndex]
  
  if (!state.isRecording) {
    console.log(`[ë…¹ìŒ ì¢…ë£Œ ì‹¤íŒ¨] ${state.name}ë‹˜ì˜ ë…¹ìŒì´ ì§„í–‰ ì¤‘ì´ ì•„ë‹™ë‹ˆë‹¤.`)
    return
  }
  
  if (!recorderMap.value[state.id]) {
    console.error(`[ë…¹ìŒ ì¢…ë£Œ ì‹¤íŒ¨] ${state.name}ë‹˜ì˜ MediaRecorderê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.`)
    return
  }
  
  try {
    const recorder = recorderMap.value[state.id].mediaRecorder
    recorder.stop()
    state.isRecording = false
    console.log(`[ë…¹ìŒ ì¢…ë£Œ] ${state.name}ë‹˜ì˜ ë…¹ìŒì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.`)
  } catch (error) {
    console.error(`[ë…¹ìŒ ì¢…ë£Œ ì‹¤íŒ¨] ${state.name}ë‹˜ì˜ ë…¹ìŒ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:`, error.message)
  }
}

onMounted(async () => {
  console.log('=== PoseMiniWidget ì»´í¬ë„ŒíŠ¸ ë§ˆìš´íŠ¸ ì‹œì‘ ===')

  try {
    console.log('face-api.js ëª¨ë¸ ë¡œë”© ì‹œì‘...')
    
    // ëª¨ë¸ ë¡œë”© ì „ ìƒíƒœ í™•ì¸
    if (!faceapi.nets.tinyFaceDetector.isLoaded) {
      await faceapi.nets.tinyFaceDetector.loadFromUri('/models/tiny_face_detector')
    }
    
    if (!faceapi.nets.faceLandmark68Net.isLoaded) {
      await faceapi.nets.faceLandmark68Net.loadFromUri('/models/face_landmark_68')
    }
    
    if (!faceapi.nets.faceExpressionNet.isLoaded) {
      await faceapi.nets.faceExpressionNet.loadFromUri('/models/face_expression')
    }
    
    console.log('ëª¨ë“  face-api.js ëª¨ë¸ ë¡œë”© ì™„ë£Œ')

    // ë¹„ë””ì˜¤ ì—˜ë¦¬ë¨¼íŠ¸ ì´ˆê¸°í™”
    try {
      while (!video.value) {
        await new Promise(r => setTimeout(r, 100))
      }

      const stream = await navigator.mediaDevices.getUserMedia({ 
        video: { 
          width: 1280, 
          height: 720,
          facingMode: 'user'
        } 
      })
      
      video.value.srcObject = stream
      
      // ë¹„ë””ì˜¤ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ í›„ ìº”ë²„ìŠ¤ í¬ê¸° ë™ê¸°í™”
      await new Promise((resolve, reject) => {
        if (!video.value) {
          reject(new Error('ë¹„ë””ì˜¤ ì—˜ë¦¬ë¨¼íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.'))
          return
        }
        video.value.onloadedmetadata = () => {
          console.log('ë¹„ë””ì˜¤ ì‹¤ì œ í•´ìƒë„:', video.value.videoWidth, video.value.videoHeight)
          // ğŸ’¡ ì‹¤ì œ ë¹„ë””ì˜¤ í•´ìƒë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ canvas í•´ìƒë„ ì„¤ì • (ìŠ¤ì¼€ì¼ë§ ë¬¸ì œ í•´ê²°)
          const width = video.value.videoWidth
          const height = video.value.videoHeight
          canvas.value.width = width
          canvas.value.height = height
          
          // Canvas ìŠ¤íƒ€ì¼ ë™ì  ì„¤ì •
          canvas.value.style.zIndex = '100'
          canvas.value.style.position = 'absolute'
          canvas.value.style.top = '0'
          canvas.value.style.left = '0'
          canvas.value.style.width = '100%'
          canvas.value.style.height = '100%'
          canvas.value.style.pointerEvents = 'none'
          canvas.value.style.background = 'transparent'
          
          console.log(`Canvas í•´ìƒë„ ë™ê¸°í™” ì™„ë£Œ: ${width}x${height}`)
          
          // í…ŒìŠ¤íŠ¸ìš© ë¹¨ê°„ ì‚¬ê°í˜• ê·¸ë¦¬ê¸°
          const ctx = canvas.value.getContext('2d')
          ctx.fillStyle = 'red'
          ctx.fillRect(20, 20, 50, 50)
          
          resolve()
        }
        video.value.onerror = reject
      })

    } catch (error) {
      console.error('ë¹„ë””ì˜¤ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜:', error)
      throw error
    }

    const analyze = async () => {
      if (!active) {
        return
      }

      try {
        const ctx = canvas.value.getContext('2d')
        const width = canvas.value.width
        const height = canvas.value.height
        
        ctx.clearRect(0, 0, width, height)
        ctx.drawImage(video.value, 0, 0, width, height)

        // í…ŒìŠ¤íŠ¸ìš© ì  ì°ê¸° (ê³„ì† ê·¸ë¦¬ê¸°)
        ctx.fillStyle = 'red'
        ctx.fillRect(10, 10, 10, 10)
        ctx.fillRect(width - 20, height - 20, 10, 10) // ìš°í•˜ë‹¨ì—ë„ ì  ì°ê¸°

        let detections = await faceapi.detectAllFaces(video.value, new faceapi.TinyFaceDetectorOptions())
          .withFaceLandmarks()
          .withFaceExpressions()
        
        // ë©´ì ‘ì ìˆ˜ì— ë”°ë¼ ê°ì§€ëœ ì–¼êµ´ ìˆ˜ ì œí•œ
        detections = detections.slice(0, props.intervieweeNames.length)
        detections.sort((a, b) => a.detection.box.x - b.detection.box.x)

        // ì–¼êµ´ ëœë“œë§ˆí¬/ë°•ìŠ¤ ì‹œê°í™” ë° ìƒíƒœ ì—…ë°ì´íŠ¸
        for (let k = 0; k < detections.length; k++) {
          const det = detections[k]
          const color = k === 0 ? 'lime' : k === 1 ? 'yellow' : 'aqua'
          
          // faceStates ì•ˆì „ì„± ì²´í¬
          if (!faceStates.value[k]) {
            continue
          }
          const faceState = faceStates.value[k]
          
          // ì–¼êµ´ ëœë“œë§ˆí¬ ì‹œê°í™”
          for (const pt of det.landmarks.positions) {
            ctx.beginPath()
            ctx.arc(pt.x, pt.y, 2.2, 0, 2 * Math.PI)
            ctx.fillStyle = color
            ctx.fill()
          }
          
          // ì–¼êµ´ ë°•ìŠ¤ ì‹œê°í™”
          const box = det.detection.box
          ctx.strokeStyle = color
          ctx.lineWidth = 2
          ctx.strokeRect(
            box.x,
            box.y,
            box.width,
            box.height
          )
          
          // ë©´ì ‘ì ì´ë¦„ í‘œì‹œ
          ctx.font = 'bold 20px sans-serif'
          ctx.fillStyle = color
          ctx.fillText(
            faceState.name,
            box.x,
            box.y - 8
          )

          // ì…ë²Œë¦¼ ê°ì§€ ë° ë…¹ìŒ ì²˜ë¦¬
          const isSpeaking = detectSpeaking(det.landmarks)
          
          if (isSpeaking) {
            if (!faceState.speaking) {
              console.log(`[ì…ë²Œë¦¼ ê°ì§€] ${faceState.name}ë‹˜ì´ ë§í•˜ê¸° ì‹œì‘í–ˆìŠµë‹ˆë‹¤.`)
            }
            faceState.speaking = true
            faceState.mouthClosedStartTime = null
            if (!faceState.isRecording) {
              startRecording(k)
            }
          } else if (faceState.speaking) {
            if (!faceState.mouthClosedStartTime) {
              faceState.mouthClosedStartTime = Date.now()
            } else if (Date.now() - faceState.mouthClosedStartTime >= MOUTH_CLOSED_THRESHOLD) {
              console.log(`[ë…¹ìŒ ì¢…ë£Œ] ${faceState.name}ë‹˜ì´ 3ì´ˆ ë™ì•ˆ ë§í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.`)
              faceState.speaking = false
              faceState.mouthClosedStartTime = null
              stopRecording(k)
            }
          }

          // í‘œì • ê°ì§€ ë° ì¹´ìš´íŠ¸
          const expLabel = Object.entries(det.expressions)
            .reduce((max, cur) => cur[1] > max[1] ? cur : max)[0]
          const expKor = expKorean[expLabel]
          if (expKor && expList.includes(expKor)) {
            faceState.expression[expKor]++
            faceState.expressionTotal++
            faceState.lastExpression = expKor
          }
        }

      } catch (error) {
        console.error('analyze í•¨ìˆ˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜:', error)
      }

      requestAnimationFrame(analyze)
    }

    analyze()
    console.log('=== PoseMiniWidget ì»´í¬ë„ŒíŠ¸ ë§ˆìš´íŠ¸ ì™„ë£Œ ===')

  } catch (error) {
    console.error('PoseMiniWidget ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ:', error)
    // ì‚¬ìš©ìì—ê²Œ ì˜¤ë¥˜ ì•Œë¦¼
    alert('ì¹´ë©”ë¼ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ê±°ë‚˜ ì¹´ë©”ë¼ ê¶Œí•œì„ í™•ì¸í•´ì£¼ì„¸ìš”.')
  }

  // 1ì´ˆë§ˆë‹¤ ë°ì´í„° ì—…ë°ì´íŠ¸ ë° ì „ì†¡
  updateInterval = setInterval(() => {
    const currentData = {}
    faceStates.value.forEach((state, index) => {
      const id = props.intervieweeIds[index]
      // ëˆ„ì ê°’ í•©ì‚°
      const acc = accumulatedNonverbalData.value[id]
      // í‘œì • ëˆ„ì  í•©ì‚°
      const expHistory = acc?.facial_expression_history || []
      const sumExp = expHistory.reduce((acc, cur) => {
        acc.smile += cur.smile || 0
        acc.neutral += cur.neutral || 0
        acc.frown += cur.frown || 0
        acc.angry += cur.angry || 0
        return acc
      }, { smile: 0, neutral: 0, frown: 0, angry: 0 })
      // ìì„¸ ëˆ„ì  í•©ì‚°
      const postureHistory = acc?.posture_history || []
      const sumPosture = postureHistory.reduce((acc, cur) => {
        acc.upright += cur.upright || 0
        acc.leaning += cur.leaning || 0
        acc.slouching += cur.slouching || 0
        return acc
      }, { upright: 0, leaning: 0, slouching: 0 })
      // ë§ˆì§€ë§‰ timestamp
      const lastTimestamp = expHistory.length > 0 ? expHistory[expHistory.length-1].timestamp : Date.now()
      currentData[id] = {
        posture: sumPosture,
        facial_expression: sumExp,
        gaze: 0, // ëˆ„ì  ì‹œì„ /ì œìŠ¤ì²˜ í•„ìš”ì‹œ ì¶”ê°€
        gesture: 0,
        timestamp: lastTimestamp
      }
    })
    nonverbalData.value = currentData
    emit('updateNonverbalData', currentData)
  }, 1000)
})

onBeforeUnmount(() => {
  active = false
  console.log('[ì»´í¬ë„ŒíŠ¸ ì •ë¦¬] PoseMiniWidget ì»´í¬ë„ŒíŠ¸ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤...')
  
  // ëª¨ë“  ë©´ì ‘ìì˜ ë…¹ìŒ ì¤‘ì§€ ë° ë¦¬ì†ŒìŠ¤ ì •ë¦¬
  Object.entries(recorderMap.value).forEach(([id, recorderData]) => {
    if (recorderData.mediaRecorder && recorderData.mediaRecorder.state !== 'inactive') {
      console.log(`[ê°•ì œ ì¢…ë£Œ] ë©´ì ‘ì ID ${id}ì˜ ë…¹ìŒì„ ê°•ì œ ì¢…ë£Œí•©ë‹ˆë‹¤.`)
      try {
        recorderData.mediaRecorder.stop()
      } catch (error) {
        console.warn(`[ë¦¬ì†ŒìŠ¤ ì •ë¦¬] ë©´ì ‘ì ID ${id}ì˜ ë…¹ìŒ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜:`, error.message)
      }
    }
    
    // ìŠ¤íŠ¸ë¦¼ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    if (recorderData.stream) {
      recorderData.stream.getTracks().forEach(track => {
        track.stop()
      })
    }
  })
  
  // recorderMap ì´ˆê¸°í™”
  recorderMap.value = {}
  
  console.log('[ì»´í¬ë„ŒíŠ¸ ì •ë¦¬ ì™„ë£Œ] PoseMiniWidget ì»´í¬ë„ŒíŠ¸ê°€ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.')

  if (updateInterval) {
    clearInterval(updateInterval)
  }
})
</script>
